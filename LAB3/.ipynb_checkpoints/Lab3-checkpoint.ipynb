{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "## Motion Capture Clustering\n",
    "\n",
    "### Team members: Luay Dajani, Dana Geislinger, Chris Morgan, Caroll Rodriguez\n",
    "##### Github - https://github.com/cdmorgan103/7331DataMiningNoShow\n",
    "\n",
    "MSDS 7331, 12/02/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset provides data on captured hand gestures using motion tracking sensors on various usersâ€™ gloves. This was done to properly model some of the different movements involved in making 5 different hand gestures (.\n",
    "\n",
    "From a business perspective, we'd like to model the different locations of the sensors and see if we can properly cluster the different sensors with enough separation relative to their x, y, and z coordinates.\n",
    "\n",
    "To measure effectiveness of our clustering, we will be examining the Silhouette for the different clustering algorithms we will examine for this dataset. While clustering could be combined with the usage of other modeling techniques, we are primarily going to be examining clustering itself for this particular dataset so we can better understand how much variance actually occurs within the data as a whole and relative to the variance caused by different users and gestures.\n",
    "\n",
    "Considering that our business user will need to better understand different positioning information for sensor utilization, this will allow stakeholders to have an additional approximation tool for determining what part of the hand is in what location of the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data is provided as a CSV file. A header provides the name of each attribute. A question mark '?' is used to indicate a missing value. A record corresponds to a single instant or frame as recorded by the camera system. Data can be found at the link below for reference and a full explanation is available in the table below as well.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Dataset\n",
    "\n",
    "| Variable Name  | Data Type | Variable Type         | Description                                                             |\n",
    "| -------------- | --------- | --------------------- | ----------------------------------------------------------------------- |\n",
    "| Class          | Nominal  | Identifier            | Classifier for hand gesture made scaled 1 to 5 (1=Fist, 2=Stop, 3=Point1 finger, 4=Point 2 fingers, 5=Grab)                                 |\n",
    "| User           | Nominal | Identifier            | ID number representing 15 different test users (scaled 0 to 14)| \n",
    "| Xi (0-11) | Interval  | Integer        | Measures distance in milimeters on the x axis for the 'i'th sensor (0-11) |\n",
    "| Yi (0-11) | Interval  | Integer        | Measures distance in milimeters on the y axis for the 'i'th sensor (0-11) |\n",
    "| Zi (0-11) | Interval  | Integer     | Measures distance in milimeters on the z axis for the 'i'th sensor (0-11) |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Because our data is 3 dimensional, a transformation of our dataset was required to adequately cluster the data in a 3 dimensional space.\n",
    "\n",
    "\n",
    "#### Final Dataset\n",
    "| Variable Name  | Data Type | Variable Type         | Description                                                             |\n",
    "| -------------- | --------- | --------------------- | ----------------------------------------------------------------------- |\n",
    "| Class          | Nominal  | Identifier            | Classifier for hand gesture made scaled 1 to 5 (1=Fist, 2=Stop, 3=Point1 finger, 4=Point 2 fingers, 5=Grab)                                 |\n",
    "| User           | Nominal | Identifier            | ID number representing 15 different test users (scaled 0 to 14)| \n",
    "| Sensor    | Nominal  | Identifier       | Sensor identifier (sensors labeled 0-11) |\n",
    "| X         | Interval  | Integer        | Measures distance in milimeters on the x axis for sensor location |\n",
    "| Y         | Interval  | Integer        | Measures distance in milimeters on the y axis  for sensor location |\n",
    "| Z         | Interval  | Integer     | Measures distance in milimeters on the z axis  for sensor location|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Each record is a set. The i-th marker of a given record does not necessarily correspond to the i-th marker of a different record, so it is possible for sensors to be in different positions from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "To get started, we'll load in our dataset and check the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78096 entries, 0 to 78095\n",
      "Data columns (total 38 columns):\n",
      "Class    78096 non-null int64\n",
      "User     78096 non-null int64\n",
      "X0       78096 non-null float64\n",
      "Y0       78096 non-null float64\n",
      "Z0       78096 non-null float64\n",
      "X1       78096 non-null float64\n",
      "Y1       78096 non-null float64\n",
      "Z1       78096 non-null float64\n",
      "X2       78096 non-null float64\n",
      "Y2       78096 non-null float64\n",
      "Z2       78096 non-null float64\n",
      "X3       78096 non-null object\n",
      "Y3       78096 non-null object\n",
      "Z3       78096 non-null object\n",
      "X4       78096 non-null object\n",
      "Y4       78096 non-null object\n",
      "Z4       78096 non-null object\n",
      "X5       78096 non-null object\n",
      "Y5       78096 non-null object\n",
      "Z5       78096 non-null object\n",
      "X6       78096 non-null object\n",
      "Y6       78096 non-null object\n",
      "Z6       78096 non-null object\n",
      "X7       78096 non-null object\n",
      "Y7       78096 non-null object\n",
      "Z7       78096 non-null object\n",
      "X8       78096 non-null object\n",
      "Y8       78096 non-null object\n",
      "Z8       78096 non-null object\n",
      "X9       78096 non-null object\n",
      "Y9       78096 non-null object\n",
      "Z9       78096 non-null object\n",
      "X10      78096 non-null object\n",
      "Y10      78096 non-null object\n",
      "Z10      78096 non-null object\n",
      "X11      78096 non-null object\n",
      "Y11      78096 non-null object\n",
      "Z11      78096 non-null object\n",
      "dtypes: float64(9), int64(2), object(27)\n",
      "memory usage: 22.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from pprint import pprint\n",
    "#from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_columns\",100)\n",
    "\n",
    "# Load the data into variable 'df'\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/cdmorgan103/7331DataMiningNoShow/master/LAB3/Postures.csv')\n",
    "\n",
    "# Get an overview of the raw data\n",
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>User</th>\n",
       "      <th>X0</th>\n",
       "      <th>Y0</th>\n",
       "      <th>Z0</th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Z1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Z2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Z3</th>\n",
       "      <th>X4</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Z4</th>\n",
       "      <th>X5</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Z5</th>\n",
       "      <th>X6</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Z6</th>\n",
       "      <th>X7</th>\n",
       "      <th>Y7</th>\n",
       "      <th>Z7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y8</th>\n",
       "      <th>Z8</th>\n",
       "      <th>X9</th>\n",
       "      <th>Y9</th>\n",
       "      <th>Z9</th>\n",
       "      <th>X10</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Z10</th>\n",
       "      <th>X11</th>\n",
       "      <th>Y11</th>\n",
       "      <th>Z11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.263880</td>\n",
       "      <td>71.466776</td>\n",
       "      <td>-64.807709</td>\n",
       "      <td>76.895635</td>\n",
       "      <td>42.462500</td>\n",
       "      <td>-72.780545</td>\n",
       "      <td>36.621229</td>\n",
       "      <td>81.680557</td>\n",
       "      <td>-52.919272</td>\n",
       "      <td>85.2322638852917</td>\n",
       "      <td>67.7492195028673</td>\n",
       "      <td>-73.684130041833</td>\n",
       "      <td>59.1885757027887</td>\n",
       "      <td>10.6789364098231</td>\n",
       "      <td>-71.2977813147725</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.527558</td>\n",
       "      <td>72.266609</td>\n",
       "      <td>-61.935252</td>\n",
       "      <td>39.135978</td>\n",
       "      <td>82.538530</td>\n",
       "      <td>-49.596509</td>\n",
       "      <td>79.223743</td>\n",
       "      <td>43.254091</td>\n",
       "      <td>-69.982489</td>\n",
       "      <td>87.4508729469625</td>\n",
       "      <td>68.4008083028339</td>\n",
       "      <td>-70.703990925959</td>\n",
       "      <td>61.5874515532753</td>\n",
       "      <td>11.7799190329758</td>\n",
       "      <td>-68.827417756239</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.849928</td>\n",
       "      <td>72.469064</td>\n",
       "      <td>-62.562788</td>\n",
       "      <td>37.988804</td>\n",
       "      <td>82.631347</td>\n",
       "      <td>-50.606259</td>\n",
       "      <td>78.451526</td>\n",
       "      <td>43.567403</td>\n",
       "      <td>-70.658489</td>\n",
       "      <td>86.8353875680762</td>\n",
       "      <td>68.9079249764243</td>\n",
       "      <td>-71.1383441365739</td>\n",
       "      <td>61.6864271910576</td>\n",
       "      <td>11.7934398850428</td>\n",
       "      <td>-68.88931646056</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.329647</td>\n",
       "      <td>71.707275</td>\n",
       "      <td>-63.688956</td>\n",
       "      <td>36.561863</td>\n",
       "      <td>81.868749</td>\n",
       "      <td>-52.752784</td>\n",
       "      <td>86.320630</td>\n",
       "      <td>68.214645</td>\n",
       "      <td>-72.228461</td>\n",
       "      <td>61.5961571288978</td>\n",
       "      <td>11.2506481750465</td>\n",
       "      <td>-68.9564252307431</td>\n",
       "      <td>77.3872254123912</td>\n",
       "      <td>42.7178334810919</td>\n",
       "      <td>-72.0151462991019</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  User         X0         Y0         Z0         X1         Y1  \\\n",
       "0      0     0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1      1     0  54.263880  71.466776 -64.807709  76.895635  42.462500   \n",
       "2      1     0  56.527558  72.266609 -61.935252  39.135978  82.538530   \n",
       "3      1     0  55.849928  72.469064 -62.562788  37.988804  82.631347   \n",
       "4      1     0  55.329647  71.707275 -63.688956  36.561863  81.868749   \n",
       "\n",
       "          Z1         X2         Y2         Z2                X3  \\\n",
       "0   0.000000   0.000000   0.000000   0.000000                 0   \n",
       "1 -72.780545  36.621229  81.680557 -52.919272  85.2322638852917   \n",
       "2 -49.596509  79.223743  43.254091 -69.982489  87.4508729469625   \n",
       "3 -50.606259  78.451526  43.567403 -70.658489  86.8353875680762   \n",
       "4 -52.752784  86.320630  68.214645 -72.228461  61.5961571288978   \n",
       "\n",
       "                 Y3                 Z3                X4                Y4  \\\n",
       "0                 0                  0                 0                 0   \n",
       "1  67.7492195028673   -73.684130041833  59.1885757027887  10.6789364098231   \n",
       "2  68.4008083028339   -70.703990925959  61.5874515532753  11.7799190329758   \n",
       "3  68.9079249764243  -71.1383441365739  61.6864271910576  11.7934398850428   \n",
       "4  11.2506481750465  -68.9564252307431  77.3872254123912  42.7178334810919   \n",
       "\n",
       "                  Z4 X5 Y5 Z5 X6 Y6 Z6 X7 Y7 Z7 X8 Y8 Z8 X9 Y9 Z9 X10 Y10 Z10  \\\n",
       "0                  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   0   0   0   \n",
       "1  -71.2977813147725  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   ?   ?   ?   \n",
       "2   -68.827417756239  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   ?   ?   ?   \n",
       "3    -68.88931646056  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   ?   ?   ?   \n",
       "4  -72.0151462991019  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   ?   ?   ?   \n",
       "\n",
       "  X11 Y11 Z11  \n",
       "0   0   0   0  \n",
       "1   ?   ?   ?  \n",
       "2   ?   ?   ?  \n",
       "3   ?   ?   ?  \n",
       "4   ?   ?   ?  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examination of the data shows that we have a dummy row of 0 that will need to be removed. The data also shows a significant amount of '?' for missing data points, so we will convert these values to NaN as a numeric data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will remove class 0 \n",
    "df = df[df.Class !=0 ]\n",
    "\n",
    "#change ? to none to improve data format\n",
    "df=df.replace({'?': 'NaN'})\n",
    "\n",
    "#coerces into numeric\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 78095 entries, 1 to 78095\n",
      "Data columns (total 38 columns):\n",
      "Class    78095 non-null int64\n",
      "User     78095 non-null int64\n",
      "X0       78095 non-null float64\n",
      "Y0       78095 non-null float64\n",
      "Z0       78095 non-null float64\n",
      "X1       78095 non-null float64\n",
      "Y1       78095 non-null float64\n",
      "Z1       78095 non-null float64\n",
      "X2       78095 non-null float64\n",
      "Y2       78095 non-null float64\n",
      "Z2       78095 non-null float64\n",
      "X3       77405 non-null float64\n",
      "Y3       77405 non-null float64\n",
      "Z3       77405 non-null float64\n",
      "X4       74975 non-null float64\n",
      "Y4       74975 non-null float64\n",
      "Z4       74975 non-null float64\n",
      "X5       65072 non-null float64\n",
      "Y5       65072 non-null float64\n",
      "Z5       65072 non-null float64\n",
      "X6       52247 non-null float64\n",
      "Y6       52247 non-null float64\n",
      "Z6       52247 non-null float64\n",
      "X7       38943 non-null float64\n",
      "Y7       38943 non-null float64\n",
      "Z7       38943 non-null float64\n",
      "X8       30563 non-null float64\n",
      "Y8       30563 non-null float64\n",
      "Z8       30563 non-null float64\n",
      "X9       23967 non-null float64\n",
      "Y9       23967 non-null float64\n",
      "Z9       23967 non-null float64\n",
      "X10      14752 non-null float64\n",
      "Y10      14752 non-null float64\n",
      "Z10      14752 non-null float64\n",
      "X11      31 non-null float64\n",
      "Y11      31 non-null float64\n",
      "Z11      31 non-null float64\n",
      "dtypes: float64(36), int64(2)\n",
      "memory usage: 23.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Get an overview of the raw data\n",
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 31 data points appear for the 11th sensor, and are present for one user only. We will delete this as a feature as it does not provide any significant value given lack of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['X11']\n",
    "del df['Y11']\n",
    "del df['Z11']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformatting the Data for Sensor Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for this dataset is formatted with each capture being treated as a separate observation. This makes sense if you are classifying the gesture or user, but we are trying to cluster in order to identify which sensor on the glove corresponds to a specific set of coordinates. Therefore, we must stack the data in order to get a new dataset where each observation has only a single X, Y, and Z coordinate, and corresponds to the specific position of an individual sensor module in any given capture. \n",
    "\n",
    "The original dataset consists of 78,095 captures with coordinates on up to 11 sensors in any given capture, as well as metadata pertaining to the capture about the user and class the capture was representative of. We will retain this metadata, and also create a new variable that corresponds to the sensor number (0 through 10). However, each coordinate will now be labelled as the raw coordinate number on each cartesian axis (X, Y, Z) regardless of sensor number (as opposed to X0, Y10, Z5, etc. as was the case originally).\n",
    "\n",
    "At this point, we will now also drop missing datapoints; NaN values in this dataset represent sensors that were obscured or not recorded for some other reason during a capture, and since we are interested in clustering by sensor, it does not further the analysis to include sensor coordinates for which no data was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of dataframes with X, Y, Z coordinates selected for each individual sensor (0-10)\n",
    "df_coords = [df.loc[:, ['Class', 'User', 'X%d' % i, 'Y%d' %i, 'Z%d' % i]] for i in range(11)]\n",
    "\n",
    "for i in range(len(df_coords)):\n",
    "    # NaN values are dropped at this time\n",
    "    df_coords[i] = df_coords[i].dropna()\n",
    "    \n",
    "    # Coordinate variable labels will now be standardized\n",
    "    df_coords[i].columns = ['Class', 'User', 'X', 'Y', 'Z']\n",
    "    \n",
    "    # Sensor number will now be added as a variable 'sensor' to each dataset\n",
    "    df_coords[i]['Sensor'] = i\n",
    "    \n",
    "# Combine datasets into new dataframe: df2\n",
    "df2 = pd.concat(df_coords, ignore_index=True)\n",
    "\n",
    "# Reorder columns with metadata columns first\n",
    "df2 = df2[['Class', 'User', 'Sensor', 'X', 'Y', 'Z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly formatted dataset, we now have 612,209 distinct cartesian coordinates with corresponding Sensor, Class, and User metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 612209 entries, 0 to 612208\n",
      "Data columns (total 6 columns):\n",
      "Class     612209 non-null int64\n",
      "User      612209 non-null int64\n",
      "Sensor    612209 non-null int64\n",
      "X         612209 non-null float64\n",
      "Y         612209 non-null float64\n",
      "Z         612209 non-null float64\n",
      "dtypes: float64(3), int64(3)\n",
      "memory usage: 28.0 MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: [1, 2, 3, 4, 5]\n",
      "User: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Total records: 612209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Print descriptive info for the unique values for each predictor\n",
    "\n",
    "print('Class:', list(df2.Class.unique()))\n",
    "print('User:', list(df2.User.unique()))\n",
    "#df_User = df.groupby(['User','Class'])['Class'].count() \n",
    "df_Class = df2.groupby(['Class', 'User'])['User'].count()\n",
    "\n",
    "df_Class['total'] = len(df2) #df_User \n",
    "print('Total records:', len(df2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of Datapoints per User by Class (Hand Gesture)\n",
    "Examining the dataset in a stratified manner shows that there is clearly an unequal distribution of hand gestures relative to users and records for each User as a whole. While this is a bit concerning in some extreme cases, it shouldn't drastically change our results and shouldn't require and further stratification or cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Class</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8998</td>\n",
       "      <td>16425</td>\n",
       "      <td>9782</td>\n",
       "      <td>11786</td>\n",
       "      <td>20274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6369</td>\n",
       "      <td>8184</td>\n",
       "      <td>5391</td>\n",
       "      <td>4754</td>\n",
       "      <td>10733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5987</td>\n",
       "      <td>8876</td>\n",
       "      <td>3605</td>\n",
       "      <td>4738</td>\n",
       "      <td>10806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>597</td>\n",
       "      <td>612</td>\n",
       "      <td>965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4604</td>\n",
       "      <td>10841</td>\n",
       "      <td>6471</td>\n",
       "      <td>8204</td>\n",
       "      <td>11819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5016</td>\n",
       "      <td>10582</td>\n",
       "      <td>5318</td>\n",
       "      <td>6168</td>\n",
       "      <td>11702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>432</td>\n",
       "      <td>1016</td>\n",
       "      <td>806</td>\n",
       "      <td>1033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6450</td>\n",
       "      <td>15242</td>\n",
       "      <td>9724</td>\n",
       "      <td>8603</td>\n",
       "      <td>12984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4560</td>\n",
       "      <td>9201</td>\n",
       "      <td>5421</td>\n",
       "      <td>7495</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15045</td>\n",
       "      <td>16291</td>\n",
       "      <td>14964</td>\n",
       "      <td>13461</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9581</td>\n",
       "      <td>14372</td>\n",
       "      <td>9552</td>\n",
       "      <td>14000</td>\n",
       "      <td>16734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4183</td>\n",
       "      <td>10322</td>\n",
       "      <td>6240</td>\n",
       "      <td>7491</td>\n",
       "      <td>8205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11882</td>\n",
       "      <td>16177</td>\n",
       "      <td>7797</td>\n",
       "      <td>12521</td>\n",
       "      <td>15177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7394</td>\n",
       "      <td>19672</td>\n",
       "      <td>15857</td>\n",
       "      <td>8374</td>\n",
       "      <td>9924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Class      1      2      3      4      5\n",
       "User                                    \n",
       "0       8998  16425   9782  11786  20274\n",
       "1       6369   8184   5391   4754  10733\n",
       "2       5987   8876   3605   4738  10806\n",
       "4        114    597    612    965      0\n",
       "5       4604  10841   6471   8204  11819\n",
       "6       5016  10582   5318   6168  11702\n",
       "7        432   1016    806   1033      0\n",
       "8       6450  15242   9724   8603  12984\n",
       "9       4560   9201   5421   7495   8305\n",
       "10     15045  16291  14964  13461  16000\n",
       "11      9581  14372   9552  14000  16734\n",
       "12      4183  10322   6240   7491   8205\n",
       "13     11882  16177   7797  12521  15177\n",
       "14      7394  19672  15857   8374   9924"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_color_table = pd.crosstab(index=df2[\"User\"], \n",
    "                          columns=df2[\"Class\"])\n",
    "\n",
    "class_color_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below show the distribution of data points by user and hand gestures. Users 4 & 7 have the lowest number of data points. User 10 had the most total of datapoints recorded in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17ab9bfff28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_color_table.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more detailed look at each Class and User combination displaying the number of datapoints. This just further iterates the unequal distribution in parts of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_Class['total']\n",
    "#plot and format bar chart\n",
    "plt.figure(figsize=(16,5), dpi=80)\n",
    "_ = df_Class.plot(kind='bar')\n",
    "#plt.barh(df_neighborhood, width=0.4, height=0.4, align='center', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Plotting of User/Class Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set up plotly API key\n",
    "plotly.tools.set_credentials_file(username='cdmorgan103', api_key='bTdTqEvg0T0FhRUwW3Cr')\n",
    "\n",
    "# Enable offline plots (embed into ipynb)\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Define list of possible users\n",
    "possible_users = set(df.User.unique())\n",
    "\n",
    "# Define dictionary of all possible user/class combinations (which ones took pictures of which)\n",
    "possible_classes = {usr: set(df.where(df.User == usr).Class.dropna().unique()) for usr in possible_users}\n",
    "\n",
    "# Define functions to create 3D-interactive plots of each user/position combo\n",
    "def parse_coords(cls=1, usr=1):\n",
    "    \"\"\" Helper function: return all coords for specified User and Class\n",
    "    \n",
    "    cls (int): Class (defaults to 1)\n",
    "    usr (int): User (defaults to 1)\n",
    "    \n",
    "    Ret (dict): {dimension (str): coordinates (pd.Series)}\n",
    "    \"\"\"\n",
    "    dims = ['X', 'Y', 'Z']\n",
    "    fill = lambda suffix: [suffix + str(i) for i in range(1,11)]\n",
    "    cols = {col: fill(col) for col in dims}\n",
    "    coords = {dim: df[cols[dim]].where(df.User == usr).where(df.Class == cls).stack() for dim in dims}\n",
    "    return coords\n",
    "\n",
    "def plot_3d(cls=1, usr=1, title=None, allpts=False, clusters=np.array([0])):\n",
    "    \"\"\" Return 3D interactive plot for given coordinates set (single user/sign combos)\n",
    "    \n",
    "    cls (int): Class (defaults to 1)\n",
    "    usr (int): User (defaults to 1)\n",
    "    title (str): Title of the plot (defaults to \"User {usr} Position {cls}\")\n",
    "    allpts (bool): Whether or not this is meant to cluster all points in the dataset (df2)\n",
    "        All 600k+ points is too many to plot interactively; this will sample 20,000 points randomly and graph them\n",
    "    clusters (np.array): Clusters to use for color-coding (only valid for allpts at this time)\n",
    "    \n",
    "    Ret (plotly.graph_objs.Scatter3d figure object)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure user/class designation is valid\n",
    "    if not usr in possible_users:\n",
    "        return \"ERROR: User %d does not have any data. Try again with a different user specified.\" % usr\n",
    "    elif not cls in possible_classes[usr]:\n",
    "        return \"ERROR: User %d does not have any data for Class %d. Try again with a different class specified.\" % (usr, cls)\n",
    "    \n",
    "    # Process points to draw\n",
    "    if allpts:\n",
    "        # Sample 20,000 random coords w/o replacement\n",
    "        coords = df2.copy()\n",
    "        if clusters.any():\n",
    "            coords['cluster'] = clusters\n",
    "        coords = coords.sample(n=20000, replace=False)\n",
    "    else:\n",
    "        coords = parse_coords(cls, usr)\n",
    "        \n",
    "    # Name plot if title not provided (default behavior)\n",
    "    if not title:\n",
    "        if allpts:\n",
    "            title = \"20,000 Random Points from Dataset\"\n",
    "            if clusters.any():\n",
    "                title += \" (Clustered)\"\n",
    "        else:\n",
    "            title = \"User %d Position %d\" % (usr, cls)\n",
    "\n",
    "    # Color-code clustered points, if provided\n",
    "    if allpts and clusters.any():\n",
    "        marker_params = dict(size=5,\n",
    "                             color=coords.cluster,\n",
    "                             colorscale='Portland'\n",
    "                             )\n",
    "    else:\n",
    "        marker_params = dict(size=5)\n",
    "            \n",
    "    data = [go.Scatter3d(x=coords['X'],\n",
    "                         y=coords['Y'],\n",
    "                         z=coords['Z'],\n",
    "                         mode='markers',\n",
    "                         marker=marker_params\n",
    "                         )\n",
    "            ]\n",
    "\n",
    "    layout = dict(width=800,\n",
    "                  height=700,\n",
    "                  autosize=True,\n",
    "                  title=title,\n",
    "                  scene=dict(xaxis=dict(gridcolor='rgb(255, 255, 255)',\n",
    "                                        zerolinecolor='rgb(255, 255, 255)',\n",
    "                                        showbackground=True,\n",
    "                                        backgroundcolor='rgb(230, 230,230)'\n",
    "                                        ),\n",
    "                             yaxis=dict(gridcolor='rgb(255, 255, 255)',\n",
    "                                        zerolinecolor='rgb(255, 255, 255)',\n",
    "                                        showbackground=True,\n",
    "                                        backgroundcolor='rgb(230, 230,230)'\n",
    "                                        ),\n",
    "                             zaxis=dict(gridcolor='rgb(255, 255, 255)',\n",
    "                                        zerolinecolor='rgb(255, 255, 255)',\n",
    "                                        showbackground=True,\n",
    "                                        backgroundcolor='rgb(230, 230,230)'\n",
    "                                        ),\n",
    "                             camera=dict(up=dict(x=0,\n",
    "                                                 y=0,\n",
    "                                                 z=1\n",
    "                                                 ),\n",
    "                                         eye=dict(x=-1.7428,\n",
    "                                                  y=1.0707,\n",
    "                                                  z=0.7100,\n",
    "                                                  )\n",
    "                                         ),\n",
    "                             aspectratio = dict(x=1,\n",
    "                                                y=1,\n",
    "                                                z=0.7\n",
    "                                                ),\n",
    "                             aspectmode = 'manual'\n",
    "                             ),\n",
    "                )\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename=\"Hands-%s\" % title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~cdmorgan103/0 or inside your plot.ly account where it is named 'Hands-User 1 Position 1'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~cdmorgan103/0.embed\" height=\"700px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_3d(cls=1, usr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, we will be attempting to cluster the data on the basis of the sensors used to collect the data. As described in the business understanding section, this dataset consists of snapshot measurements taken from a special glove fitted with sensors to track a person's hand movements and map them to 3-dimensional space. The dataset has recorded data for up to 11 sensors in each capture (some may have been obscured when measurement was performed), and associated 3D coordinates for each sensor.\n",
    "\n",
    "Our goal is to cluster each data point based on which of the 11 possible sensors it corresponds to. Each record in the dataset has been formatted so that it represents a single coordinate location for an individual sensor. Unfortunately, the number designation of each sensor varies by instance, so X0 in one capture might be tracking a completely different sensor than X0 in another capture. Therefore, we will need to use clustering without a known designation for each sensor. To accomplish this, we will attempt clustering with 11 total clusters defined in our clustering algorithms, ideally corresponding with each of the sensors that is present on the glove. We will begin by performing cluster analysis of all points aggregated together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data set (shown in Modeling and Evaluation 3), it seems that while there are some obvious discrete clusters in 3-dimensional space these clusters are not necessarily uniform, globular clusters. These types of clusters might be well defined using Gaussian mixture model clustering which would allow for normally distributed clusters of varying shapes and sizes. We will first attempt to fit a Gaussian mixture model to obtain clusters for the 11 sensors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Fit 11 clusters\n",
    "#  Set random state to '123456789' to make sure same result achieved each time\n",
    "gmm = GMM(n_components=11,\n",
    "          random_state=123456789\n",
    "          )\n",
    "gmm_X = df2[['X','Y','Z']]\n",
    "gmm.fit(gmm_X)\n",
    "gmm_clusters = gmm.predict(gmm_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this unoptimized GMM model as a baseline, we will now adjust parameters in an attempt to optimize the fit of our clusters. Perhaps the most important parameter available for GMM is covariance type which controls how covariances are treated between clusters. By default, 'full' is used which allows each cluster its own general covariance matrix. This is a good baseline and may prove to be the best method for our data, but since the sensor clustering patterns should be relatively similar it may be beneficial to test another level for the parameter. To this end, we will test 'tied', in which each component shares the same covariance matrix but it is still defined as a general covariance matrix. The other alternatives are 'diagonal' and 'spherical', but these constrain the type of covariance matrix that is possible and we do not believe this would provide a good fit for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters using 'tied' covariance type\n",
    "gmm_tied = GMM(n_components=11,\n",
    "          random_state=123456789,\n",
    "          covariance_type='tied'\n",
    "          )\n",
    "gmm_X_tied = df2[['X','Y','Z']]\n",
    "gmm_tied.fit(gmm_X_tied)\n",
    "gmm_clusters_tied = gmm_tied.predict(gmm_X_tied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the produced silhouette score using the 'tied' parameter (shown in Modeling and Evaluation 2), it seems that this covariance type may in fact be beneficial to our clusters. Since this was successful, we will also test 'sphericity' and 'diagonal' to be sure that we are using the optimal method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters using 'spherical' covariance type\n",
    "gmm_sphere = GMM(n_components=11,\n",
    "          random_state=123456789,\n",
    "          covariance_type='spherical'\n",
    "          )\n",
    "gmm_X_sphere = df2[['X','Y','Z']]\n",
    "gmm_sphere.fit(gmm_X_sphere)\n",
    "gmm_clusters_sphere = gmm_sphere.predict(gmm_X_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters using 'diag' covariance type\n",
    "gmm_diag = GMM(n_components=11,\n",
    "          random_state=123456789,\n",
    "          covariance_type='diag'\n",
    "          )\n",
    "gmm_X_diag = df2[['X','Y','Z']]\n",
    "gmm_diag.fit(gmm_X_diag)\n",
    "gmm_clusters_diag = gmm_diag.predict(gmm_X_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had predicted, the spherical and diagonal covariance types were too constraining to our clusters and both negatively impacted our silhouette score. Nevertheless, we can now proceed with confidence that the 'tied' covariance type (a shared general covariance matrix) is optimal for our sensor clusters. This parameter will be used for subsequent GMM clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter that may benefit our process in init_params. There must be initial weights, averages, and precisions applied to the dataset before the GMM algorithm can correctly start clustering. This initiation is by default performed by a single round of k-means clustering (with the round specified by n_init).\n",
    "\n",
    "We will test these parameters to see if they increase our silhouette score at all. First, we begin by testing random initiation instead of one based on k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters with random initiation\n",
    "gmm_rand = GMM(n_components=11,\n",
    "               random_state=123456789,\n",
    "               covariance_type='tied',\n",
    "               init_params='random'\n",
    "               )\n",
    "gmm_X_rand = df2[['X','Y','Z']]\n",
    "gmm_rand.fit(gmm_X_rand)\n",
    "gmm_clusters_rand = gmm_rand.predict(gmm_X_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate benefit of using random initialization is that the processing time decreased significantly, likely because the initial k-means requires substantial processing that can be mitigated by using a random initialization instead. This is interesting because it suggests that the GMM algorithm itself is quick, but that the initial k-means requires a long time to compute.\n",
    "\n",
    "Regardless, changing from a single k-means initialization to a random one dropped our silhouette score substantially (from 0.27 to 0.021, shown in Modeling and Evaluation 2), so we will not be using this method going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameter of interest is in some ways the opposite of the previous test; we would now like to see if multiple initiation cycles with k-means will produce a better score than only one. The GMM function will pick the most beneficial initiation out of all the iterations the user specifies. For this test, we will test 3 iterations against our previous GMM of a single k-means iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters with 3 k-means init\n",
    "gmm_3init = GMM(n_components=11,\n",
    "               random_state=123456789,\n",
    "               covariance_type='tied',\n",
    "               init_params='kmeans',\n",
    "               n_init=3\n",
    "          )\n",
    "gmm_X_3init = df2[['X','Y','Z']]\n",
    "gmm_3init.fit(gmm_X_3init)\n",
    "gmm_clusters_3init = gmm_3init.predict(gmm_X_3init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test took slightly longer than the previous clustering steps, as the GMM clustering step took time to process the 3 k-means iterations. It also markedly improved our best score (from 0.27 to almost 0.3, a 10% increase shown in Modeling and Evaluation 2), so we will finally test the same model building step but this time with 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit 11 clusters with 3 k-means init\n",
    "gmm_10init = GMM(n_components=11,\n",
    "               random_state=123456789,\n",
    "               covariance_type='tied',\n",
    "               init_params='kmeans',\n",
    "               n_init=10\n",
    "          )\n",
    "gmm_X_10init = df2[['X','Y','Z']]\n",
    "gmm_10init.fit(gmm_X_10init)\n",
    "gmm_clusters_10init = gmm_10init.predict(gmm_X_10init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 k-means iterations, this model took considerably longer to cluster, but did not increase our score over 3 iterations at all. This means that the initiation that resulted in the best initial k-means clustering over 10 iterations was also found within the first 3 iterations (at least for the random sample we tested). Therefore, we conclude that 3 iterations of k-means for initialization is ideal for our purposes, since the processing increase is relatively minor but the increase in efficacy is substantial.\n",
    "\n",
    "In the following section, we will quantitatively analyze our clustering fits with the previously described parameter adjustments and conclude with what GMM clustering model performed the best.\n",
    "\n",
    "Overall, we have found GMM to be a robust clustering algorithm that seems to produce meaningful clusters without much optimization, but slight optimizations can increase the effectiveness of the clustering as shown in our previous tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering\n",
    "Means identifies clusters depending on the number and the clusters required.  KMeans goal is to classify 'n' observations into 'k' cluster using centroids which are the calculated means by selected observations.  We are using around 612209 with 11 clusters, with each point (observations) classified by its closest calculated Centroid.\n",
    "\n",
    "As per the tests below, as with GMM previously, we will test a couple of different optimal parameter configurations and then compare to each other and against the GMM clusters using the silhouette score from Modeling and Evaluation 2.\n",
    "\n",
    "KMeans is easy to execute but has a lot of parameters with supposedly optimal The paremeters of interest are as follows:\n",
    "\n",
    "n_clusters: This will remain 11 as this corresponds to the number of sensors:\n",
    "\n",
    "* init:\n",
    "    * k-means++: to increase efficiency, try to guess the centroids and work from there\n",
    "    * random: initially try to ramdomly place centroids, then start\n",
    "* n_init: number of times the kmeans will be run against different centroid seeds then pick the best one.  In this case will try 1 for efficiency and then 5 for better results (default is 10, but after testing did not pay to go more than five, also each interation takes around 5-7 seconds, so 10 interations could take 1 minute)\n",
    "* tol: reletive tolerace to declare convergence of a point within a cluster.  This will be tested at \n",
    "* random_state = 1: This will remain the same to keep make the guessing of the clusters deterministic\n",
    "* All others will remain as their default as they do not seem to have relevance with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up results matrix\n",
    "df_kms_result = pd.DataFrame(\n",
    "{'Configuration':['Config 1','Config 2','Config 3','Config 4','Config 5','Config 6'],\n",
    "'SilhouetteScore':[None,None,None,None,None,None],\n",
    "'RelativeTime':[None,None,None,None,None,None]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 1: 'init' parameter set to k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 1\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, ShuffleSplit\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms = KMeans(n_clusters=11, init='k-means++',random_state=1, n_init=5, tol=.0001)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms.fit(kms_X)\n",
    "kms_clusters = kms.predict(kms_X)\n",
    "kms_clusters\n",
    "\n",
    "df_kms_result.loc[0, 'RelativeTime'] = 'Medium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what is generally the default case for (except less n_iter), this turns out to be a good enough base case in terms of time and result (silhouette scores in Modeling and Evaluation 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 2: 'init' parameter set to random (and coincidentally n_init set to 5 to compare wit the next configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 2\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms2 = KMeans(n_clusters=11, init='random', random_state=1, n_init=5, tol=.0001)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms2.fit(kms_X)\n",
    "kms_clusters2 = kms2.predict(kms_X)\n",
    "kms_clusters2\n",
    "df_kms_result.loc[1, 'RelativeTime'] = 'Medium High'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, due to a less efficient algorithm, changing init from k-means++ to random adds almost 15 seconds extra, nevertheless, this does add to the silhouette score (as shown later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 3: 'n_init' parameter set to 1 (with init = k-means++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 3\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms3 = KMeans(n_clusters=11, init='k-means++', random_state=1, n_init=1, tol=.0001)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms3.fit(kms_X)\n",
    "kms_clusters3 = kms3.predict(kms_X)\n",
    "kms_clusters3\n",
    "df_kms_result.loc[2, 'RelativeTime'] = 'Very Short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the n_init seriously decreases processing time and is the most significant factor in respect to time. For this scenario, reducing from 5 to 1 reduced the seconds from around 25s to 5s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 4: 'n_init' parameter set to 1 (with init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 4\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms4 = KMeans(n_clusters=11, init='random', random_state=1, n_init=1, tol=.0001)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms4.fit(kms_X)\n",
    "kms_clusters4 = kms4.predict(kms_X)\n",
    "kms_clusters4\n",
    "df_kms_result.loc[3, 'RelativeTime'] = 'Short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting to random from k-means++ again had the expected result of slightly increasing completion time, but in this case only from 5s to 10s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 5: tol as default higher than default (using best so far 'n_init' parameter set to 1 & init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 5\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms5 = KMeans(n_clusters=11, init='random', random_state=1, n_init=1, tol=.1)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms5.fit(kms_X)\n",
    "kms_clusters5 = kms5.predict(kms_X)\n",
    "kms_clusters5\n",
    "df_kms_result.loc[4, 'RelativeTime'] = 'Very Short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number (reducing the toleration) seems to decrease time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 6: tol as default lower than default (using best so far 'n_init' parameter set to 1 & init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# config 6\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit 11 clusters\n",
    "kms6 = KMeans(n_clusters=11, init='random', random_state=1, n_init=1, tol=.0000001)\n",
    "kms_X = df2[['X','Y','Z']]\n",
    "kms6.fit(kms_X)\n",
    "kms_clusters6 = kms6.predict(kms_X)\n",
    "kms_clusters6\n",
    "df_kms_result.loc[5, 'RelativeTime'] = 'Medium Low'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering the tol (increasing the actual tolerance) increases the time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now attempt to quantify our clustering in some way so that it can be compared to other clustering methods. While the dataset does include labels for each sensor (X0, X1, X2, etc.), it is stated in the data description that the numbering is arbitrary; sensor X0 in one instance could be tracking a completely different sensor than it is in any other instance. Therefore, we do not know the true sensor identity at any given time and cannot concretely compare against the known sensor identities with a metric such as the adjusted rand score.\n",
    "\n",
    "Nevertheless, a situation such as this is the perfect application for clustering. For this project we will rely on the silhouette metric, which is a measure of the closeness of a point to the average at the center of its cluster combined with the smallest distance between the same point and the next closest cluster. This metric allows us to quantify the successfulness of our clustering without having to rely on true known measurements. The silhouette score is optimal at 1.0, so higher values are desired. In our case, our 600,000+ records proved too computationally intensive to calculate by force, so a random sample of 50,000 is taken instead. The final metric we be an average of the silhouette score for each of the 50,000 points randomly sampled from our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Clustering\n",
    "This metric will allow us to compare the efficacy of GMM to other clustering methods as well as fine-tune the parameters of each cluster model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "gmm_sil = silhouette_score(gmm_X, gmm_clusters, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM Silhouette Score:\", gmm_sil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As briefly discussed in Modeling and Evaluation 1, we noted a slight increase (0.265 to 0.274) in switching from 'full' to 'tied' covariance type, which changes our model from using general covariance matrices for each cluster (full) to using a single general covariance matrix for the entire clustering (tied). We also tested 'spherical' and 'diagonal' types, which constrain the shared covariance matrix to the aforementioned type of matrix, but this did not improve our silhouette score (shown below). The ranking of covariance types in terms of silhouette score was tied > full > spherical > diagonal. We will use 'tied' going forward for using GMM clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_tied = silhouette_score(gmm_X_tied, gmm_clusters_tied, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (tied) Silhouette Score:\", gmm_sil_tied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_sphere = silhouette_score(gmm_X_sphere, gmm_clusters_sphere, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (spherical) Silhouette Score:\", gmm_sil_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_diag = silhouette_score(gmm_X_diag, gmm_clusters_diag, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (diagonal) Silhouette Score:\", gmm_sil_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, next we tested random instantiation of our initial clusters as opposed to k-means initialization. This ran very quickly, but was detrimental to our silhouette score, producing the lowest value for GMM clustering that we have seen, 0.0216."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_rand = silhouette_score(gmm_X_rand, gmm_clusters_rand, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (tied, random initialization) Silhouette Score:\", gmm_sil_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our silhouette score without k-means initialization was so poor, we reverted to using k-means. However, we next wanted to test if we could improve our clustering fit by performing more iterations of k-means during initialization. We initially tested 3 iterations, and then 10 to see if we could further improve the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_3init = silhouette_score(gmm_X_3init, gmm_clusters_3init, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (3 k-means iterations) Silhouette Score:\", gmm_sil_3init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get silhouette score\n",
    "gmm_sil_10init = silhouette_score(gmm_X_10init, gmm_clusters_10init, sample_size=50000, random_state=123456789)\n",
    "print(\"Average GMM (10 k-means iterations) Silhouette Score:\", gmm_sil_10init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to significantly improve our silhouette score by introducing 3 iterations, but 10 iterations resulted in the same score so we decided that 3 iterations were adequate for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_sils = (gmm_sil, gmm_sil_tied, gmm_sil_sphere, gmm_sil_diag, gmm_sil_rand, gmm_sil_3init, gmm_sil_10init)\n",
    "names = ('Initial',\n",
    "         'Tied Covariance',\n",
    "         'Spherical Covariance',\n",
    "         'Diagonal Covariance',\n",
    "         'Random Initialization',\n",
    "         '3 Iteration Initialization (Final)',\n",
    "         '10 Iteration Initialization'\n",
    "        )\n",
    "gmm_sil_dict = {names[i]: gmm_sils[i] for i in range(len(gmm_sils))}\n",
    "gmm_sil_table = pd.DataFrame.from_dict(gmm_sil_dict, orient='index', columns=['Slihouette Score'])\n",
    "\n",
    "gmm_sil_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the preceding tests, our optimized GMM clustering model has the following parameters:\n",
    "- **tied** covariance type\n",
    "- **kmeans** initiation parameter\n",
    "- **3** initialization iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm_X_final = gmm_X_3init.copy()\n",
    "gmm_clusters_final = gmm_clusters_3init\n",
    "gmm_sil_final = gmm_sil_3init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 1: 'init' parameter set to k-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 1\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms_sil = silhouette_score(kms_X, kms_clusters, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms_sil)\n",
    "df_kms_result.loc[0, 'SilhouetteScore'] = kms_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this desically default configuration, thoough we would like to be closer to 1, the result is already coming out a slight but higher than GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 2: 'init' parameter set to random (and coincidentally n_init set to 5 to compare wit the next configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 2\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms2_sil = silhouette_score(kms_X, kms_clusters2, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms2_sil)\n",
    "df_kms_result.loc[1, 'SilhouetteScore'] = kms2_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by changing to random the score increased slightly.  Again that is because this option does not sacrifice a small amount of accuracy for efficiency.  Onthe othe hand, if this were a longer dataset with more iterations, the trad-off may not be viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 3: 'n_init' parameter set to 1 (with init = k-means++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 3\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms3_sil = silhouette_score(kms_X, kms_clusters3, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms3_sil)\n",
    "df_kms_result.loc[2, 'SilhouetteScore'] = kms3_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, reducing the iterations also reduces the potential of finding a higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 4: 'n_init' parameter set to 1 (with init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 4\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms4_sil = silhouette_score(kms_X, kms_clusters4, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms4_sil)\n",
    "df_kms_result.loc[3, 'SilhouetteScore'] = kms4_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction in n_init in the case where init = random did not at all reduce the score, but significantly reduced the time.  This is a case where you can have your cake and eat it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 5: tol as default higher than default (using best so far 'n_init' parameter set to 1 & init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 5\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms5_sil = silhouette_score(kms_X, kms_clusters5, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms5_sil)\n",
    "df_kms_result.loc[4, 'SilhouetteScore'] = kms5_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here reducing tolerance reduces the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration 6: tol as default lower than default (using best so far 'n_init' parameter set to 1 & init = random as this turned out better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config 6\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Get silhouette score\n",
    "kms6_sil = silhouette_score(kms_X, kms_clusters6, sample_size=50000, random_state=1)\n",
    "print(\"Average GMM Silhouette Score:\", kms6_sil)\n",
    "df_kms_result.loc[5, 'SilhouetteScore'] = kms6_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here increasing tolerance does increase the score over config 5, but not over the default config. So it looks like the default tolerance is the best, and going higher or lower will reduce the score either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_kms_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the results, config 4 (n_init = 1, init = random, tol = 0.0001 (default)) was the most accurate and still very efficient configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown below are 2 random samplings of 20,000 points each from the dataset. This was done to illustrate that when multiple random samples are plotted in 3-dimensional space, the distributions are slightly different each time but there are similar patterns that can be observed consistently such as 2 large ellipsoid clusters on the right side and some smaller clusters on the left side. This provides some anecdotal evidence that clustering may be successful using all available data aggregated from every user and every gesture class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(allpts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(allpts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Silhouette Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Split into X Features & Y Training\n",
    "df2b = df2.copy()\n",
    "#df2b = df2b[df2b.User > 12 ] #User 13 & 14\n",
    "#df3 = df2b[['X','Y','Z']]\n",
    "#X = df3.values[:,]\n",
    "df3s = df2b[['Sensor']]\n",
    "#y = df3s.values[:,] \n",
    " \n",
    "X = kms_X.values[:,]\n",
    "y = df3s[['Sensor']]\n",
    "clusterer = kms\n",
    "sample_size=50000 \n",
    "random_state=123456789\n",
    "silhouette_avg = kms_sil\n",
    "\n",
    "range_n_clusters = [11]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a rsample_size=50andom generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = clusterer #KMeans(n_clusters=11, init='k-means++',random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_avg\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GMM Silhouette Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Split into X Features & Y Training\n",
    "df2b = df2.copy()\n",
    "#df2b = df2b[df2b.User > 12 ] #User 13 & 14\n",
    "#df3 = df2b[['X','Y','Z']]\n",
    "#X = df3.values[:,]\n",
    "df3s = df2b[['Sensor']]\n",
    "#y = df3s.values[:,] \n",
    " \n",
    "X = gmm_X.values[:,]\n",
    "y = df3s[['Sensor']]\n",
    "clusterer = gmm\n",
    "sample_size=50000 \n",
    "random_state=123456789\n",
    "silhouette_avg = gmm_sil\n",
    "\n",
    "range_n_clusters = [11]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax2) = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    #ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    #ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a rsample_size=50andom generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = clusterer #gmm(n_clusters=11, init='k-means++',random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = gmm_sil #silhouette_score(X, cluster_labels, sample_size=sample_size, random_state=random_state)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    #ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    #ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    #ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    #ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    #ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.means_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for GMM clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will plot our GMM produced clusters using the original, unoptimized clustering fit to establish a baseline of what unoptimized GMM clustering looks like for our data in 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = gmm_X['Z'].astype(float)\n",
    "xdata = gmm_X['X'].astype(float)\n",
    "ydata = gmm_X['Y'].astype(float)\n",
    "_ = ax.scatter3D(gmm_X['X'], gmm_X['Y'], gmm_X['Z'], c=gmm_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(allpts=True, clusters=gmm_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the basic GMM clusters above, both as a static image consisting of all points (more than 600,000) and an interactive plot that is sampled as 20,000 random points from the dataset color-coded by cluster. This sampling was necessary to produce an interactive 3D plot since a higher number of points rendered much too slowly in the browser. We believe that an interactive plot like this is crucial for modeling spatial 3D data effectively. From these visuals, we can see that there does appear to be decent clustering.\n",
    "\n",
    "From the data description, we know that there are 2 sensors on each finger and 3 sensors on the thumb. We also know that there are several positions in which multiple (sometimes more than half) of the sensors are obscured from view and not recorded. Therefore, this clustering makes sense from a logical perspective, but it could likely still be improved.\n",
    "\n",
    "Another interesting comparison is the distribution of clusters as defined by GMM. While we do not know from the raw data which sensors correspond to which point on the glove, we do know that in a majority of recorded instances there are sensors that were unable to be recorded. It would be interesting to note if this clustering algorithm created clusters of varying sizes, which might be indicative of an uneven distribution of which sensors were not recorded most or least often.\n",
    "\n",
    "Next, we take a look at how the clustering has distributed our dataset with this initial GMM approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_gmm = gmm_X.copy()\n",
    "df_gmm['Clusters'] = gmm_clusters\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_gmm.groupby('Clusters').count()['X'] / len(gmm_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each GMM-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note from this distribution that there are significantly varying distributions between clusters. If these clusters do in fact coincide with sensors on the glove, this could provide evidence that certain sensors appear more often than others.\n",
    "\n",
    "This also makes logical sense, since it would be expected that some of the gestures being performed would have obscured at least part of the hand. The distribution shows that from GMM, there are 2 clusters defined that are very scarce in the total (both less than 5%) compared to the rest. This could be accounted for as the 2 sensors present on a finger that is often obscured, such as the pinky finger. Conversely, there are 2 clusters that stand out as about the same percentage as each other (almost 14%) but both significantly higher than most of the other clusters. These cluster could represent sensors on fingers that are needed to perform most of the gestures recorded, so they might show up regardless of user or class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot our clusters from the final, optimized GMM clustering that we decided on based on silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(allpts=True, clusters=gmm_clusters_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, these clusters appear similar to our initial GMM clusters, which is good since those made intuitive sense when viewed. Most clusters now appear to more evenly distributed, which is likely due to our switch from 'full' to 'tied' covariance as these clusters are now all using the same shared covariance matrix, but there do also appear to be 1 or 2 clusters that are much smaller than the others that were not as fragmented in our unoptimized clustering.\n",
    "\n",
    "We will also re-plot our clustering distribution to see if how that has changed, and if the more even clusters we expect from the visual analysis are truly represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_gmm_final = gmm_X_final.copy()\n",
    "df_gmm_final['Clusters'] = gmm_clusters_final\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts_final = df_gmm_final.groupby('Clusters').count()['X'] / len(gmm_X_final) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts_final.plot(kind='bar', title=\"Percentage of Each GMM-Defined Cluster (Final Optimization)\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our cluster distribution changed dramatically, and not exactly in the way that was expected; the optimized clustering did seem to \"smooth\" out our cluster distribution for those clusters in the middle, but it significantly reduced the size of clusters 6, 7 and 10, with only one significantly large cluster defined (cluster 3).\n",
    "\n",
    "Even though this runs somewhat counter to our initial logic, this clustering method had a significantly better silhouette score than our initial fit. Furthermore, if we assume that each cluster is representative of one of the 11 possible sensors, it makes sense that there would be multiple sensors with poor representation, since from the initial dataset we know that many captures were missing many sensor position readings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the clusters of the GMM model built with a single random initialization, to get a visual representation of a clustering model with a very poor silhouette score (0.0216)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_3d(allpts=True, clusters=gmm_clusters_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clustering is in stark contrast to our good GMM clusters. There are 3 large clusters represented and multiple small, insignificant ones. With this as an example of a poor clustering fit, we are more confident in the positive qualities of our previously finalized GMM clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visuals allow us to glean important insights about our data and to visualize the effectiveness of our GMM produced clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeanse Clustering\n",
    "\n",
    "First, we will plot our GMM produced clusters using the default clustering fit (Config 1) to establish a baseline of what GMM clustering looks like for our data in 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config 1\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(dpi=50)\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = kms_X['Z'].astype(float)\n",
    "xdata = kms_X['X'].astype(float)\n",
    "ydata = kms_X['Y'].astype(float)\n",
    "ax.scatter3D(kms_X['X'], kms_X['Y'], kms_X['Z'], c=kms_clusters, s=1)\n",
    "ax.set_title('Gesture points for X, Y, Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 1\n",
    "plot_3d(allpts=True, clusters=kms_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then against our best one for config 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config 4\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(dpi=50)\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = kms_X['Z'].astype(float)\n",
    "xdata = kms_X['X'].astype(float)\n",
    "ydata = kms_X['Y'].astype(float)\n",
    "ax.scatter3D(kms_X['X'], kms_X['Y'], kms_X['Z'], c=kms_clusters4, s=1)\n",
    "ax.set_title('Gesture points for X, Y, Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 4\n",
    "plot_3d(allpts=True, clusters=kms_clusters4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per points distribution between the different configuraitons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 1\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 1)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 2\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters2\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 2)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 3\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters3\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 3)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 4\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters4\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 4)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 5\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters5\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 5)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Config 6\n",
    "# Create a new dataframe with the clusters defined as a new variable\n",
    "df_kms = kms_X.copy()\n",
    "df_kms['Clusters'] = kms_clusters6\n",
    "\n",
    "# Count the number of instances in each cluster and convert to a percentage of the total\n",
    "cluster_pcts = df_kms.groupby('Clusters').count()['X'] / len(kms_X) * 100\n",
    "\n",
    "# Plot the distribution\n",
    "ax = cluster_pcts.plot(kind='bar', title=\"Percentage of Each KMeans (Config 6)-Defined Cluster\")\n",
    "_ = ax.set_ylabel(\"Percentage of Total Points (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defauls or config 1 was relitively smooth in the middle 9 categories but had almost nothing for the first and last.  Configs 2, 4, and 6 had a very similar looking likely due to it being random and similar toleranc.  Config 3 and 5 look almost the same, though the parameters were very differnet.  Maybe this is because a low tolerance for 'random' is similar to a default or high tolerance for 'k-means++'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Clustering method | y        | z              | Comment                                 |\n",
    "| ----------------- | ---------| -------------- | --------------------------------------- |\n",
    "| Kmeans            | value  | value            | Classifier for hand gesture ma          |\n",
    "| GMM               | value  | value            | Classifier for hand gesture ma          |\n",
    "| Class             | value  | value            | Classifier for hand gesture ma          |\n",
    "| Class             | value  | value            | Classifier for hand gesture ma          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examination of our various clustering models shows somewhat below the hoped-for results. While we were able to create clusters for our dataset that appear reasonable, there is clearly too much variation in the data for the selected clustering algorithms to consistently create a clustered region that could be usable for most motion capture-oriented applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion capture requires a great deal of precision for users to feel immersion or a useful effect, so a very low precision cluster indicated by silhouette scores in the .3's as mentioned in the Modeling and Evaluation 4 section, indicates that other modeling techniques would need to be utilized, likely in conjunction with some of the clustering work performed in this effort.\n",
    "\n",
    "Other than a cursory reference to common areas of sensor location, these results will not have an effective business application on their own and we would strongly advise against deployment until more mechanisms are added to create more significant performance results. This would likely only be achieved by using clustering in conjunction with other techniques such as using a random forest model to predict location based on known gestures or known behavior by supplementing this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to Center Data\n",
    "The data in this dataset is taken with slightly differing points serving as the origin for each coordinate set. Due to this fact, we attempted to center our data somewhat by subtracting the minimum x,y,&z coordinate value for each coordinate set to effectively 0 each record so a similar origin could be created. \n",
    "\n",
    "We had hoped that this would allow us to reduce variance and keep points on a more even plane. Unfortunately, the centering method was not successful and we ended up with a lower silhouette score than was obtained otherwise. \n",
    "\n",
    "Nevertheless, we believe that it was incredibly important to the data cleaning process to rule out any potential for improving our dataset prior to clustering, and this method allowed us to rule out one potential change that could have been made to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data centering\n",
    "This section searches for minimum x,y,z coordinate values and zeroes them and adjusts each coordinate accordingly to reduce variation relative to starting hand position. First we will spit the data into new dataframes for the x,y,z coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates new sub dataframes for x y and z coordinates\n",
    "dfx=df[['X0','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']].copy()\n",
    "dfy=df[['Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10']].copy()\n",
    "dfz=df[['Z0','Z1','Z2','Z3','Z4','Z5','Z6','Z7','Z8','Z9','Z10']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next We will calculate the minimum value for each coordinate's data frame and store it in the respective dataframe. We can then iterate through each column and subtract the minimum value from each respective column to 0 our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculates min value\n",
    "dfx['xmin']=dfx.min(axis=1)\n",
    "dfy['ymin']=dfy.min(axis=1)\n",
    "dfz['zmin']=dfz.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###takes minimum and subtracts form all\n",
    "for a in range(11): \n",
    "    dfx['X%d' % a] = dfx['X%d' % a] - dfx['xmin']\n",
    "    \n",
    "for b in range(11): \n",
    "    dfy['Y%d' % b] = dfy['Y%d' % b] - dfy['ymin']\n",
    "    \n",
    "for c in range(11): \n",
    "    dfz['Z%d' % c] = dfz['Z%d' % c] - dfz['zmin']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will remove the calculated fields and merge our new dataframes together into a new central dataframe. Finally we will reorder the columns to be consistent with the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the calculated min values for merging\n",
    "del dfx['xmin']\n",
    "del dfy['ymin']\n",
    "del dfz['zmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create mini dataframe and then concatenate the transformed dataframes\n",
    "dfmini = df.iloc[:,0:2].copy()\n",
    "dfnew= pd.concat([dfmini,dfx, dfy,dfz], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reorder the columns\n",
    "dfnew = dfnew[['Class','User', 'X0','Y0','Z0','X1','Y1','Z1','X2','Y2','Z2','X3','Y3','Z3','X4','Y4','Z4','X5','Y5','Z5',\n",
    "               'X6','Y6','Z6','X7','Y7','Z7','X8','Y8','Z8','X9','Y9','Z9','X10','Y10','Z10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately testing showed a much worse clustering score via silhouette scoring at nearly .06 in our analysis and examination showed more work would need to be examined to make this possible. This is likely due to the fact that our dataset post transformation was heavily overlapped at the axis points for zeroing which took away some of the natural variation that should be present and effectively overcorrected for the problem we were experiencing.\n",
    "\n",
    "We do feel the approach however, could be worth investigating further to greater improve model performance.\n",
    "\n",
    "As an area of future work, attempting to account for variance in how far the min value was away from the other points could be utilized to help center and keep the data from pooling at axis points lowering our ability to cluster. This may be a matter of attempting to normalize and center values while examining min max values where hand position clearly started outside of a typical origin location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional 3D plots\n",
    "In this section we performed further 3d plotting to improve our understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1=Fist(with thumb out), \n",
    "#2=Stop(hand flat), \n",
    "#3=Point1(point with pointer finger), \n",
    "#4=Point2(point with pointer and middle fingers), \n",
    "#5=Grab(fingers curled as if to grab). \n",
    "\n",
    "#filter certain users for plotting below\n",
    "df_user6 = df[df.User == 6][df.Class == 2]\n",
    "df_user14 = df[df.User == 14][df.Class == 1]\n",
    "df_user5 = df[df.User == 5][df.Class == 4]\n",
    "df_user8 = df[df.User == 8][df.Class == 3]\n",
    "\n",
    "#One row in data table - represnts on set of data points\n",
    "df_row12000 = df[11999:12000][df.Class == 5]\n",
    "\n",
    "# feature categories\n",
    "X_features = ['X0','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']\n",
    "Y_features = ['Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10']\n",
    "Z_features = ['Z0','Z1','Z2','Z3','Z4','Z5','Z6','Z7','Z8','Z9','Z10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One record plotted in 3D show Class 5 (Grab Gestures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#one set of markers\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = df_row12000[Z_features].astype(float)\n",
    "xdata = df_row12000[X_features].astype(float)\n",
    "ydata = df_row12000[Y_features].astype(float)\n",
    "ax.scatter3D(xdata, ydata, zdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All User 6's records plotted in 3D with Posture 3 (Stop sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = df_user6[Z_features].astype(float)\n",
    "xdata = df_user6[X_features].astype(float)\n",
    "ydata = df_user6[Y_features].astype(float)\n",
    "ax.scatter3D(xdata, ydata, zdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All User 14's records plotted in 3D with Class 1 (Fist Gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = df_user14[Z_features].astype(float)\n",
    "xdata = df_user14[X_features].astype(float)\n",
    "ydata = df_user14[Y_features].astype(float)\n",
    "ax.scatter3D(xdata, ydata, zdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All User 8's records plotted in 3D with Class 3 (point with pointer finger Gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = df_user8[Z_features].astype(float)\n",
    "xdata = df_user8[X_features].astype(float)\n",
    "ydata = df_user8[Y_features].astype(float)\n",
    "ax.scatter3D(xdata, ydata, zdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All User 5's records plotted in 3D with Class 5 (Grab Gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "zdata = df_user5[Z_features].astype(float)\n",
    "xdata = df_user5[X_features].astype(float)\n",
    "ydata = df_user5[Y_features].astype(float)\n",
    "ax.scatter3D(xdata, ydata, zdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All User 5's records plotted in 3D in a Wireframe show Class 2 (Stop Gesture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_wireframe(df_user6[X_features], df_user6[Y_features], df_user6[Z_features], color='black')\n",
    "ax.set_title('wireframe');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Work (save for putting plots in later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "In this example the silhouette analysis is used to choose an optimal value for n_clusters. The silhouette plot shows that the n_clusters value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.\n",
    "\n",
    "Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when n_clusters is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the n_clusters is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labelled scatter plot on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Split into X Features & Y Training\n",
    "df2b = df2.copy()\n",
    "df2b = df2b[df2b.User > 12 ] #User 13 & 14\n",
    "df3 = df2b[['X','Y','Z']]\n",
    "X = df3.values[:,]\n",
    "df3s = df2b[['Sensor']]\n",
    "y = df3s.values[:,] \n",
    " \n",
    "\n",
    "# Not done for now\n",
    "range_n_clusters = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=11, init='k-means++',random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
