{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 Accuracies (All Data):<br>\n",
    "Logistic 0.79806744073009<br>\n",
    "Random Trees 0.7796558116889997<br>\n",
    "XGBoost 0.7974703138481137<br>\n",
    "<br>\n",
    "Task 2 Accuracies (All Data):<br>\n",
    "Logistic 0.6499769288124675<br>\n",
    "Random Trees 0.6465841776412136<br>\n",
    "XGBoost 0.669085391786314<br>\n",
    "<br>\n",
    "\n",
    "Based on the previous accuracies calculated against all data in the dataset, the logistic and XGBoost models both performed better than random forest for task 1 (predicting NoShows). For task 2 (predicting Gender), XGBoost performed better than the others. This is an interesting pattern to observe, since both tasks involved the classification of a binary response variable, so we had expected that intuitively both tasks would perform the same relative to each model.\n",
    "\n",
    "Nevertheless, it is important to verify these results statistically rather than just by anecdotal observation. To do this, we will calculate 95% confidence intervals for the difference in accuracy between the 2 models, as was described in the Unit 7.5 asynchronous materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for Task 1 (NoShow Prediction)\n",
      "Logistic vs. RandomForest: [-0.021813441476899117, -0.015009816605281449]\n",
      "Logistic vs. XGBoost: [-0.003946004672715851, 0.0027517509087634155]\n",
      "RandomForest vs. XGBoost: [0.014410873524419308, 0.021218130793808822]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate confidence interval for accuracy between 2 models \n",
    "import math\n",
    "def diff_ci(acc1, acc2, n1=110527, n2=110527):\n",
    "    \"\"\" Get a 95% CI of the difference in accuracy between 2 models given accuracy and n for each model.\"\"\"\n",
    "    e1 = 1.0 - float(acc1)\n",
    "    e2 = 1.0 - float(acc2)\n",
    "    var1 = (e1 * float(acc1)) / float(n1)\n",
    "    var2 = (e2 * float(acc2)) / float(n2)\n",
    "    ci_half_width = 1.96 * math.sqrt(var1 + var2)\n",
    "    e_diff = e1 - e2\n",
    "    ci = [e_diff - ci_half_width, e_diff + ci_half_width]\n",
    "    return ci\n",
    "\n",
    "# Task 1\n",
    "print(\"Accuracies for Task 1 (NoShow Prediction)\")\n",
    "print(\"Logistic vs. RandomForest:\", diff_ci(0.79806744073009, 0.7796558116889997))\n",
    "print(\"Logistic vs. XGBoost:\", diff_ci(0.79806744073009, 0.7974703138481137))\n",
    "print(\"RandomForest vs. XGBoost:\", diff_ci(0.7796558116889997, 0.7974703138481137))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above confidence intervals, Logistic and XGBoost are both significantly different in terms of prediction accuracy than RandomForest at 95% confidence level. However, XGBoost and Logistic both provide prediction accuracy that is not statistically different from each other at 95% confidence for task 1. Therefore, we can say that either XGBoost or Logistic regression is suitable for predicting NoShows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for Task 2 (Patient Gender Prediction)\n",
      "Logistic vs. RandomForest: [-0.007373949611068579, 0.0005884472685607246]\n",
      "Logistic vs. XGBoost: [0.015158391709425633, 0.02305853423826736]\n",
      "RandomForest vs. XGBoost: [0.018546718709783583, 0.026455709580417264]\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "print(\"Accuracies for Task 2 (Patient Gender Prediction)\")\n",
    "print(\"Logistic vs. RandomForest:\", diff_ci(0.6499769288124675, 0.6465841776412136))\n",
    "print(\"Logistic vs. XGBoost:\", diff_ci(0.6499769288124675, 0.669085391786314))\n",
    "print(\"RandomForest vs. XGBoost:\", diff_ci(0.6465841776412136, 0.669085391786314))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 2 (predicting Gender), Logistic and RandomForest were not statistically significantly different at 95% confidence level. However, XGBoost was shown to be more accurate than both models with 95% confidence.\n",
    "\n",
    "Based on these results, it appears that XGBoost is the best overall model for predicting either NoShow or Gender."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
