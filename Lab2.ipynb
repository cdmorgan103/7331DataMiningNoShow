{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "## Medical Patient No Show\n",
    "\n",
    "### Team members: Luay Dajani, Dana Geislinger, Chris Morgan, Caroll Rodriguez\n",
    "##### Github - https://github.com/cdmorgan103/7331DataMiningNoShow\n",
    "\n",
    "MSDS 7331, 10/28/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "import seaborn as sns; sns.set(font_scale=1.2)\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "import datetime \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sns\n",
    "\n",
    "# Hide deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Load the data into variable 'df' from pickled object\n",
    "from funcs import load_df\n",
    "df = load_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1\n",
    "#### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "**Meetings Notes: 10/24 ** Chris - copy from MiniLab >> Define and prepare your class variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Dataset\n",
    "We are using the same dataset from Lab1 and the minilab for the purposes of predicting noshow to clinical appointments. The dataset includes the additional created variables of age_range,  the split of date/time from Scheduled Day for deeper analysis, Scheduled day of week, Appointment day of week and days in advance the appointment was scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110527 entries, 0 to 110526\n",
      "Data columns (total 19 columns):\n",
      "PatientId         110527 non-null int64\n",
      "AppointmentID     110527 non-null int64\n",
      "Gender            110527 non-null category\n",
      "ScheduledDay      110527 non-null datetime64[ns]\n",
      "ScheduledTime     110527 non-null object\n",
      "AppointmentDay    110527 non-null datetime64[ns]\n",
      "Age               110527 non-null int64\n",
      "Neighbourhood     110527 non-null category\n",
      "Scholarship       110527 non-null int64\n",
      "Hypertension      110527 non-null int64\n",
      "Diabetes          110527 non-null int64\n",
      "Alcoholism        110527 non-null int64\n",
      "Handicap          110527 non-null int64\n",
      "SMSReceived       110527 non-null int64\n",
      "NoShow            110527 non-null bool\n",
      "DaysInAdvance     110527 non-null int64\n",
      "ScheduledDOW      110527 non-null category\n",
      "AppointmentDOW    110527 non-null category\n",
      "age_range         110527 non-null category\n",
      "dtypes: bool(1), category(5), datetime64[ns](2), int64(10), object(1)\n",
      "memory usage: 11.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *note on date/time*\n",
    "SciKitLearn models have difficulty working with complex objects such as datetime.datetime and datetime.time. To account for this, we will store all dates as ordinal values (the number of days since 1/1/1) and all time values as the number of elapsed seconds in the day (1:30PM = 13 * 3600 + 30 * 60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert date columns to ordinal integer values (days since 1/1/1)\n",
    "dt_ord = lambda dt: dt.toordinal()\n",
    "if 'ScheduledDay' in df:\n",
    "    df['ScheduledDayOrdinal'] = df['ScheduledDay'].apply(dt_ord)\n",
    "if 'AppointmentDay' in df:\n",
    "    df['AppointmentDayOrdinal'] = df['AppointmentDay'].apply(dt_ord)\n",
    "\n",
    "# Convert time values to seconds (total seconds since start of day)\n",
    "to_secs = lambda t: t.hour * 3600 + t.minute * 60 + t.second\n",
    "if 'ScheduledTime' in df:\n",
    "    df['ScheduledTimeSeconds'] = df['ScheduledTime'].apply(to_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScheduledDayOrdinal</th>\n",
       "      <th>ScheduledTimeSeconds</th>\n",
       "      <th>AppointmentDayOrdinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>736083</td>\n",
       "      <td>67088</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>736083</td>\n",
       "      <td>58107</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>736083</td>\n",
       "      <td>58744</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736083</td>\n",
       "      <td>62971</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>736083</td>\n",
       "      <td>58043</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ScheduledDayOrdinal  ScheduledTimeSeconds  AppointmentDayOrdinal\n",
       "0               736083                 67088                 736083\n",
       "1               736083                 58107                 736083\n",
       "2               736083                 58744                 736083\n",
       "3               736083                 62971                 736083\n",
       "4               736083                 58043                 736083"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['ScheduledDayOrdinal', 'ScheduledTimeSeconds', 'AppointmentDayOrdinal']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create dummy variables for our datsets class variables Handicap, AppointmentDOW, Scheduled DOW, Neighbourhood, and age range. We will also create the variable \"ismale\" to make dummy classification easier for gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110527 entries, 0 to 110526\n",
      "Columns: 115 entries, Age to IsMale\n",
      "dtypes: bool(1), int32(1), int64(10), uint8(103)\n",
      "memory usage: 19.8 MB\n"
     ]
    }
   ],
   "source": [
    "#Remove attributes not usefull\n",
    "\n",
    "del df['PatientId']\n",
    "del df['AppointmentID']\n",
    "\n",
    "# perform one-hot encoding of the categorical data \n",
    "tmp_df = pd.get_dummies(df.Handicap,prefix='Handicap')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.AppointmentDOW,prefix='AppointmentDOW')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.ScheduledDOW,prefix='ScheduledDOW')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.Neighbourhood,prefix='Neighbourhood')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.age_range,prefix='age_range')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Gender attribute with something slightly more intuitive and readable\n",
    "df['IsMale'] = df.Gender=='M' \n",
    "df.IsMale = df.IsMale.astype(np.int)\n",
    "\n",
    "# Now let's clean up the dataset\n",
    "if 'Gender' in df:\n",
    "    del df['Gender'] # if 'Sex' column still exists, delete it (as we created an ismale column)\n",
    "    \n",
    "if 'Handicap' in df:    \n",
    "    del df['Handicap'] # get rid of the original category as it is now one-hot encoded\n",
    "    \n",
    "if 'ScheduledDOW' in df:    \n",
    "    del df['ScheduledDOW'] # get rid of the original category as it is now one-hot encoded\n",
    "    \n",
    "if 'AppointmentDOW' in df:    \n",
    "    del df['AppointmentDOW'] # get rid of the original category as it is now one-hot encoded\n",
    "\n",
    "if 'Neighbourhood' in df:    \n",
    "    del df['Neighbourhood'] # get rid of the original category as it is now one-hot encoded\n",
    "\n",
    "if 'age_range' in df:\n",
    "    del df['age_range']\n",
    "\n",
    "if 'AppointmentID' in df:\n",
    "    del df['AppointmentID']\n",
    "    \n",
    "if 'ScheduledDay' in df:\n",
    "    del df['ScheduledDay']\n",
    "    \n",
    "if 'ScheduledTime' in df:\n",
    "    del df['ScheduledTime']\n",
    "\n",
    "if 'AppointmentDay' in df:\n",
    "    del df['AppointmentDay']\n",
    "\n",
    "# Get an overview of the raw data\n",
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "#### [5 points] Describe the ﬁnal dataset that is used for classiﬁcation/regression (include a description of any newly formed variables you created).\n",
    "**Meetings Notes 10/24:** Chris - copy from MiniLab\n",
    "\n",
    "**Meeting Notes: ????** Luay wants add a field that ranks the patient's no show history. If they showed up twice and didn't show up 1 would be 33%. Create a temp dataset to group Patient and determine the # of no shows then merge the dataset and build the ranking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the unnecessary variables and creating data variables, we're left with this final data set. \n",
    "\n",
    "\n",
    "\n",
    "| Variable Name  | Data Type | Variable Type         | Description                                                             |\n",
    "| -------------- | --------- | --------------------- | ----------------------------------------------------------------------- |\n",
    "| PatientID      | Interval  | Identifier            | Unique ID number for each patient.                                      |\n",
    "| AppointmentID  | Interval  | Identifier            | Unique ID number for each appointment.                                  |\n",
    "| Gender         | Nominal   | Binary Predictor      | Sex of the patient (Male/Female).                                       |\n",
    "| ScheduledDay   | Interval  | Date/Time Predictor   | **Date** and **Time** when the patient called to schedule their appointment. Should always be before *AppointmentDay*.                                                                         |\n",
    "| AppointmentDay | Interval  | Date Predictor        | Scheduled appointment **Date**. Appointment **Times** are not provided. |\n",
    "| Age            | Ratio     | Integer Predictor     | Age of the patient in years.                                            |\n",
    "| Neighbourhood  | Nominal   | Categorical Predictor | The neighborhood in which the appointment facility is located.          |\n",
    "| Scholarship    | Ordinal   | Boolean Predictor     | Whether or not the patient receives Bolsa Família financial aid. To receive this benefit, a patient's income must be under the poverty threshold, all children in the household must be vaccinated and regularly attending school, and mothers and children must receive routine medical care.                                    |\n",
    "| Hipertension   | Ordinal   | Boolean Predictor     | Whether or not a patient is classified as hypertensive (has high blood pressure).                                                                                                                     |\n",
    "| Diabetes       | Ordinal   | Boolean Predictor     | Whether or not a patient is diagnosed as a diabetic.                    |\n",
    "| Alcoholism     | Ordinal   | Boolean Predictor     | Whether or not a patient is classified as an alcoholic.                 |\n",
    "| Handcap        | Ordinal   | Boolean Predictor     | Whether or not a patient is diagnosed as being handicapped.             |\n",
    "| SMS_received   | Nominal   | Boolean Predictor     | Whether or not a patient received an SMS (text message) reminder for   their appointment.                                                                                                             |\n",
    "| No-show        | Nominal   | Boolean Response      | Whether or not a patient showed up for their appointment. True means they **did not** show up, False means they **did** show up.                                                                         |\n",
    "\n",
    "#### Created Variables\n",
    "| Variable Name  | Data Type | Variable Type         | Description                                                             |\n",
    "| -------------- | --------- | --------------------- | ----------------------------------------------------------------------- |\n",
    "| DaysInAdvance  | Ratio     | Integer Predictor     | Value for how many days in advance the appointment was scheduled.       |\n",
    "| ScheduledDOW   | Nominal   | Categorical Predictor | Day of the week for the day the patient scheduled the appointment.      |\n",
    "| AppointmentDOW | Nominal   | Categorical Predictor | Day of the week for patient appointment.                                |\n",
    "| ScheduledTime  | Interval  | Time Predictor        | **Time** of day when an appointment was scheduled.                      |\n",
    "| age_range | Nominal   | Categorical Predictor | Age grouping of patient consisting of child (0-17), Adult (18-65), & senior (66+).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 110,527 appointment records for clinics located across the coastal city of Vitória in Espírito Santo, Brazil. The dataset includes 11 meaningful predictors relating to each appointment and to the patient that scheduled that appointment. Unique numeric identifiers are provided for each patient as well as for each appointment. The response variable of interest for this data set, No-show, is a boolean variable denoting whether or not a patient made it to their scheduled appointment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                               int64\n",
      "Scholarship                       int64\n",
      "Hypertension                      int64\n",
      "Diabetes                          int64\n",
      "Alcoholism                        int64\n",
      "SMSReceived                       int64\n",
      "NoShow                             bool\n",
      "DaysInAdvance                     int64\n",
      "ScheduledDayOrdinal               int64\n",
      "AppointmentDayOrdinal             int64\n",
      "ScheduledTimeSeconds              int64\n",
      "Handicap_0                        uint8\n",
      "Handicap_1                        uint8\n",
      "Handicap_2                        uint8\n",
      "Handicap_3                        uint8\n",
      "Handicap_4                        uint8\n",
      "AppointmentDOW_Monday             uint8\n",
      "AppointmentDOW_Tuesday            uint8\n",
      "AppointmentDOW_Wednesday          uint8\n",
      "AppointmentDOW_Thursday           uint8\n",
      "AppointmentDOW_Friday             uint8\n",
      "AppointmentDOW_Saturday           uint8\n",
      "AppointmentDOW_Sunday             uint8\n",
      "ScheduledDOW_Monday               uint8\n",
      "ScheduledDOW_Tuesday              uint8\n",
      "ScheduledDOW_Wednesday            uint8\n",
      "ScheduledDOW_Thursday             uint8\n",
      "ScheduledDOW_Friday               uint8\n",
      "ScheduledDOW_Saturday             uint8\n",
      "ScheduledDOW_Sunday               uint8\n",
      "                                  ...  \n",
      "Neighbourhood_PRAIA DO CANTO      uint8\n",
      "Neighbourhood_PRAIA DO SUÁ        uint8\n",
      "Neighbourhood_REDENÇÃO            uint8\n",
      "Neighbourhood_REPÚBLICA           uint8\n",
      "Neighbourhood_RESISTÊNCIA         uint8\n",
      "Neighbourhood_ROMÃO               uint8\n",
      "Neighbourhood_SANTA CECÍLIA       uint8\n",
      "Neighbourhood_SANTA CLARA         uint8\n",
      "Neighbourhood_SANTA HELENA        uint8\n",
      "Neighbourhood_SANTA LUÍZA         uint8\n",
      "Neighbourhood_SANTA LÚCIA         uint8\n",
      "Neighbourhood_SANTA MARTHA        uint8\n",
      "Neighbourhood_SANTA TEREZA        uint8\n",
      "Neighbourhood_SANTO ANDRÉ         uint8\n",
      "Neighbourhood_SANTO ANTÔNIO       uint8\n",
      "Neighbourhood_SANTOS DUMONT       uint8\n",
      "Neighbourhood_SANTOS REIS         uint8\n",
      "Neighbourhood_SEGURANÇA DO LAR    uint8\n",
      "Neighbourhood_SOLON BORGES        uint8\n",
      "Neighbourhood_SÃO BENEDITO        uint8\n",
      "Neighbourhood_SÃO CRISTÓVÃO       uint8\n",
      "Neighbourhood_SÃO JOSÉ            uint8\n",
      "Neighbourhood_SÃO PEDRO           uint8\n",
      "Neighbourhood_TABUAZEIRO          uint8\n",
      "Neighbourhood_UNIVERSITÁRIO       uint8\n",
      "Neighbourhood_VILA RUBIM          uint8\n",
      "age_range_child                   uint8\n",
      "age_range_adult                   uint8\n",
      "age_range_senior                  uint8\n",
      "IsMale                            int32\n",
      "Length: 115, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 110,527 appointment records for clinics located across the coastal city of Vitória in Espírito Santo, Brazil. The dataset includes 11 meaningful predictors relating to each appointment and to the patient that scheduled that appointment. Unique numeric identifiers are provided for each patient as well as for each appointment. The response variable of interest for this data set, No-show, is a boolean variable denoting whether or not a patient made it to their scheduled appointment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Scholarship</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Alcoholism</th>\n",
       "      <th>SMSReceived</th>\n",
       "      <th>DaysInAdvance</th>\n",
       "      <th>ScheduledDayOrdinal</th>\n",
       "      <th>AppointmentDayOrdinal</th>\n",
       "      <th>ScheduledTimeSeconds</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighbourhood_SÃO CRISTÓVÃO</th>\n",
       "      <th>Neighbourhood_SÃO JOSÉ</th>\n",
       "      <th>Neighbourhood_SÃO PEDRO</th>\n",
       "      <th>Neighbourhood_TABUAZEIRO</th>\n",
       "      <th>Neighbourhood_UNIVERSITÁRIO</th>\n",
       "      <th>Neighbourhood_VILA RUBIM</th>\n",
       "      <th>age_range_child</th>\n",
       "      <th>age_range_adult</th>\n",
       "      <th>age_range_senior</th>\n",
       "      <th>IsMale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.089218</td>\n",
       "      <td>0.098266</td>\n",
       "      <td>0.197246</td>\n",
       "      <td>0.071865</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.321026</td>\n",
       "      <td>10.183792</td>\n",
       "      <td>736092.856370</td>\n",
       "      <td>736103.040162</td>\n",
       "      <td>40557.666643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.234060</td>\n",
       "      <td>0.645598</td>\n",
       "      <td>0.120342</td>\n",
       "      <td>0.350023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23.109921</td>\n",
       "      <td>0.297675</td>\n",
       "      <td>0.397921</td>\n",
       "      <td>0.258265</td>\n",
       "      <td>0.171686</td>\n",
       "      <td>0.466873</td>\n",
       "      <td>15.254924</td>\n",
       "      <td>19.140133</td>\n",
       "      <td>12.189325</td>\n",
       "      <td>11578.064436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127811</td>\n",
       "      <td>0.132541</td>\n",
       "      <td>0.147167</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.037059</td>\n",
       "      <td>0.087409</td>\n",
       "      <td>0.423412</td>\n",
       "      <td>0.478334</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>0.476979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>735912.000000</td>\n",
       "      <td>736083.000000</td>\n",
       "      <td>22176.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>736083.000000</td>\n",
       "      <td>736093.000000</td>\n",
       "      <td>30338.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>736094.000000</td>\n",
       "      <td>736102.000000</td>\n",
       "      <td>37983.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>736104.000000</td>\n",
       "      <td>736115.000000</td>\n",
       "      <td>50382.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>736123.000000</td>\n",
       "      <td>736123.000000</td>\n",
       "      <td>77255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Age    Scholarship   Hypertension       Diabetes  \\\n",
       "count  110527.000000  110527.000000  110527.000000  110527.000000   \n",
       "mean       37.089218       0.098266       0.197246       0.071865   \n",
       "std        23.109921       0.297675       0.397921       0.258265   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%        18.000000       0.000000       0.000000       0.000000   \n",
       "50%        37.000000       0.000000       0.000000       0.000000   \n",
       "75%        55.000000       0.000000       0.000000       0.000000   \n",
       "max       115.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          Alcoholism    SMSReceived  DaysInAdvance  ScheduledDayOrdinal  \\\n",
       "count  110527.000000  110527.000000  110527.000000        110527.000000   \n",
       "mean        0.030400       0.321026      10.183792        736092.856370   \n",
       "std         0.171686       0.466873      15.254924            19.140133   \n",
       "min         0.000000       0.000000       0.000000        735912.000000   \n",
       "25%         0.000000       0.000000       0.000000        736083.000000   \n",
       "50%         0.000000       0.000000       4.000000        736094.000000   \n",
       "75%         0.000000       1.000000      15.000000        736104.000000   \n",
       "max         1.000000       1.000000     179.000000        736123.000000   \n",
       "\n",
       "       AppointmentDayOrdinal  ScheduledTimeSeconds      ...        \\\n",
       "count          110527.000000         110527.000000      ...         \n",
       "mean           736103.040162          40557.666643      ...         \n",
       "std                12.189325          11578.064436      ...         \n",
       "min            736083.000000          22176.000000      ...         \n",
       "25%            736093.000000          30338.000000      ...         \n",
       "50%            736102.000000          37983.000000      ...         \n",
       "75%            736115.000000          50382.000000      ...         \n",
       "max            736123.000000          77255.000000      ...         \n",
       "\n",
       "       Neighbourhood_SÃO CRISTÓVÃO  Neighbourhood_SÃO JOSÉ  \\\n",
       "count                110527.000000           110527.000000   \n",
       "mean                      0.016611                0.017887   \n",
       "std                       0.127811                0.132541   \n",
       "min                       0.000000                0.000000   \n",
       "25%                       0.000000                0.000000   \n",
       "50%                       0.000000                0.000000   \n",
       "75%                       0.000000                0.000000   \n",
       "max                       1.000000                1.000000   \n",
       "\n",
       "       Neighbourhood_SÃO PEDRO  Neighbourhood_TABUAZEIRO  \\\n",
       "count            110527.000000             110527.000000   \n",
       "mean                  0.022148                  0.028337   \n",
       "std                   0.147167                  0.165934   \n",
       "min                   0.000000                  0.000000   \n",
       "25%                   0.000000                  0.000000   \n",
       "50%                   0.000000                  0.000000   \n",
       "75%                   0.000000                  0.000000   \n",
       "max                   1.000000                  1.000000   \n",
       "\n",
       "       Neighbourhood_UNIVERSITÁRIO  Neighbourhood_VILA RUBIM  age_range_child  \\\n",
       "count                110527.000000             110527.000000    110527.000000   \n",
       "mean                      0.001375                  0.007699         0.234060   \n",
       "std                       0.037059                  0.087409         0.423412   \n",
       "min                       0.000000                  0.000000         0.000000   \n",
       "25%                       0.000000                  0.000000         0.000000   \n",
       "50%                       0.000000                  0.000000         0.000000   \n",
       "75%                       0.000000                  0.000000         0.000000   \n",
       "max                       1.000000                  1.000000         1.000000   \n",
       "\n",
       "       age_range_adult  age_range_senior         IsMale  \n",
       "count    110527.000000     110527.000000  110527.000000  \n",
       "mean          0.645598          0.120342       0.350023  \n",
       "std           0.478334          0.325362       0.476979  \n",
       "min           0.000000          0.000000       0.000000  \n",
       "25%           0.000000          0.000000       0.000000  \n",
       "50%           1.000000          0.000000       0.000000  \n",
       "75%           1.000000          0.000000       1.000000  \n",
       "max           1.000000          1.000000       1.000000  \n",
       "\n",
       "[8 rows x 114 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple export of final dataset\n",
    "df.to_csv(\"./data/df.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1\n",
    "\n",
    "#### [10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "**Meetings Notes 10/24:** Chris - copy from MiniLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of no-show appointments and how they can impact business practicess, we must be cautious in how we score our models performance. \n",
    "\n",
    "If we were to purely prioritize accuracy, it is possible to get a optimal accuracy value with no precision relatively easily using some forms of SVM modeling with the linear kernel. This was noticed in the minilab and in practice the model predicted all patients as \"show\". While technically the best accuracy focused model, this has no business value.\n",
    "\n",
    "If we were to purely prioritize model precision however, we may find ourselves in an equally troubled situation.  A model with higher precision is desirable as it will allow us to effectively predict No-show patients with a greater degree of certainty. However, if our increased precision comes at the expense of creating too many false negative no-show patients (or a show patient who was predicted as no-show).\n",
    "\n",
    "Because of this, we need to balance a reasonable model yield that could be actionable with not creating to many false negative predictions. Therefore F1 score is likely the strongest metric for us to focus on as it will allow for a balance between precision and recall, and with the large amount of actual negatives (show patients), a F1 score is a reasonable metric to prioritize. It is critical to still consider total accuracy, precision, recall, and AUC to ensure our model does not drift to any extremes which will be our strategy for this effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2\t\n",
    "\n",
    "#### [10 points]\tChoose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "**Meeting Notes:** Dana - to figure out error with SMOTE\n",
    "Caroll to run CV on Training Set and evaluate the accuracy of Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3\n",
    "\n",
    "#### [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "**Meeting Notes**: \n",
    "**Luay (Recursive Feature Elimination for Logistic) here**\n",
    "\t1. Logistic Caroll to move from MiniLab \n",
    "\t2. RandomForest - Chris\n",
    "\t3. XGBoost/Gradient - Caroll\n",
    "\n",
    "Response Gender\n",
    "**Caroll to split training/text with Gender variable**\n",
    "**Luay (Recursive Feature Elimination for Logistic) here**\n",
    "\t1. Logistic Caroll to move from MiniLab\n",
    "\t2. Random -- Chris\n",
    "\t3. XGBoost/Gradient-- Caroll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partioning Data\n",
    "We want to forecast future NoShows based on Appointment dates. We take the data we have and build a fixed training period in the past up to the last month. The last month of data will be used as testing dataset. Before we deploy the model we will rerun the models utilizing the entire dataset to predict the future outcomes.\n",
    "This approach fits our dataset since we there is correlation between the appoointme date and whether or not they no-show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 (Response: No-Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitPerc = .3 # percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing set --- hold 1 month of data out as test set based on AppointmentDay\n",
    "df_test = df.copy()\n",
    "# Calculation to find the ordinal value of AppointmentDay to split data at\n",
    "X_test = df_test[df['AppointmentDayOrdinal'] >=  (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_test = X_test['NoShow'].values # get the labels we want.\n",
    "\n",
    "if 'NoShow' in X_test:\n",
    "    del X_test['NoShow'] # get rid of the class label\n",
    "\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy() \n",
    "                 \n",
    "X_train = df_train[df['AppointmentDayOrdinal'] < (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_train = X_train['NoShow'].values # get the labels we want.\n",
    "del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "if 'NoShow' in X_train:\n",
    "    del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "#All data\n",
    "df_tmp = df.copy()\n",
    "X = df_tmp\n",
    "y = df_tmp['NoShow']\n",
    "if 'NoShow' in X:\n",
    "    del X['NoShow'] # get rid of the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=10)\n"
     ]
    }
   ],
   "source": [
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "seed = 123456789\n",
    "cv_object = TimeSeriesSplit(n_splits=num_cv_iterations,\n",
    "                         max_train_size= None,\n",
    "                         )\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prediciting Features:  (110527, 114)\n",
      "Number of Response:  (110527,)\n",
      "Appointment Date Range (Trainig Set):  736083  -  736109\n",
      "Number of Training Records:  Predictors:  (75283, 114) Response:  (75283,)\n",
      "Number of Test Records: Predictors:  (35244, 114) Response:  (35244,)\n",
      "Appointment Date Range (Test Set):  736114  -  736123\n"
     ]
    }
   ],
   "source": [
    "print ('Number of Prediciting Features: ', X.shape)\n",
    "print ('Number of Response: ', y.shape)\n",
    "print ('Appointment Date Range (Trainig Set): ', min(X_train['AppointmentDayOrdinal']) ,' - ',  max(X_train['AppointmentDayOrdinal']))\n",
    "print ('Number of Training Records:  Predictors: ', X_train.shape, 'Response: ' ,y_train.shape)\n",
    "print ('Number of Test Records: Predictors: ',X_test.shape, 'Response: ' ,y_test.shape)\n",
    "print ('Appointment Date Range (Test Set): ', min(X_test['AppointmentDayOrdinal']) ,' - ',  max(X_test['AppointmentDayOrdinal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "<p>A grid search will help determine the optimal parameters to pass to the logistic regression function. Finding the optimal parameters will help with model prediction.</p>\n",
    "<p>Some parameters are being selected as the only option due to the type of dataset. We will try using a solver with a default parameter 'lbfgs' for binomial problems, although sag and saga are faster for larger datasets.</p>\n",
    "<p>The multi-class parameter is set to the default of 'ovr' because we have a binary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X, y, cv):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, X, y, scoring=['accuracy','precision','recall','f1']\n",
    "                            , cv=cv_object, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "    Recavg = scores['test_recall'].mean()\n",
    "    f1avg = scores['test_f1'].mean()\n",
    "\n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "    print_str3 = \"The average recall for all cv folds is: \\t\\t\\t {Recavg:.5}\"\n",
    "    print_str4 = \"The average f1 for all cv folds is: \\t\\t\\t {f1avg:.5}\"\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print(print_str3.format(Recavg=Recavg))\n",
    "    print(print_str4.format(f1avg=f1avg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "    scoresResults['Recall'] = scores['test_recall']\n",
    "    scoresResults['f1'] = scores['test_f1']\n",
    "\n",
    "    return scoresResults\n",
    "\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X, y, cv_object):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, X, y, cv=cv_object)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(y, predictions)\n",
    "    confMat = confusion_matrix(y, predictions)\n",
    "    acc = accuracy_score(y, predictions)\n",
    "    \n",
    "    print(classReport)\n",
    "    print(confMat)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon researching the possible parameters to LogisticRegression from sklearn, we decided that the most important parameters of interest were C (the inverse of regularization strength, which must be a positive floating point value) and max_iter (the maximum number of iterations for the lbfgs solver to use). The default values for these parameters are 1.0 and 100 respectively, and we have included a range of potential values for these parameters that might help improve our model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    2.6s remaining:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diplay the top model parameters\n",
    "LRestimator = regGridSearch.best_estimator_\n",
    "LRestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    2.5s remaining:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = RandomForestClassifier()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By performing a grid search, the best parameters to pass into the Logistic Regression are identified for us. Below is the best estimator parameters for class weight and cost without scaling the data. The optimal values were estimated to be 0.01 for C (100 times smaller than the default 1.0 value) and 500 for max_iter (5 times greater than the default value of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diplay the top model parameters\n",
    "RFestimator = regGridSearch.best_estimator_\n",
    "RFestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:   21.5s remaining:   14.3s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:   38.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=8, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = XGBClassifier()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diplay the top model parameters\n",
    "XGBestimator = regGridSearch.best_estimator_\n",
    "XGBestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data: \n",
      "Logistic 0.79806744073009\n",
      "Random Trees 0.7815919664412266\n",
      "XGBoost 0.7974703138481137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# All\n",
    "\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)),\n",
    "    ('Random Trees', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)),\n",
    "    ('XGBoost', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1))\n",
    "    ]\n",
    "print ('All Data: ')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1], X, y)\n",
    "\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: \n",
      "Logistic 0.7903643588971825\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print ('Training Set: ')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_train,y_train)\n",
    "      \n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "print ('Testing Set: ')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_test,y_test)\n",
    "    \n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SMOTE to perform over-sampling for balancing response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling using SMOTE to balance the NoShows\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy()\n",
    "X_train = df_train\n",
    "\n",
    "y_train = X_train['NoShow'].values # get the labels we want.\n",
    "del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "if 'NoShow' in X_train:\n",
    "    del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "sm = SMOTE()\n",
    "X_train_ovr, y_train_ovr = sm.fit_sample(X_train, y_train.ravel())\n",
    "print(\"Proportion of response in train set using SMOTE\")\n",
    "for i in np.unique(y_train) :\n",
    "    print(\"The number of {} is {} accouting for {}%.\".format(i, np.bincount(y_train)[i], np.round(np.bincount(y_train)[i]/len(y_train), 3)*100 ))\n",
    "\n",
    "for i in np.unique(y_train_ovr) :\n",
    "    print(\"The number of {} is {} accouting for {}%.\".format(i, np.bincount(y_train_ovr)[i], np.round(np.bincount(y_train_ovr)[i]/len(y_train_ovr), 3)*100 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing below that our time series is pretty well stationary over time except for Saturday which show consistantly lower no show appointments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NoShow'] = df['NoShow']\n",
    "df['NoShow_num'] = np.where(df['NoShow']==True, 1, 0)\n",
    "\n",
    "#Set dataframe with index as Appointment Day\n",
    "ts = df[['AppointmentDayOrdinal', 'NoShow'] ]\n",
    "ts = df.set_index('AppointmentDayOrdinal')\n",
    "ts.groupby(['AppointmentDayOrdinal'])['NoShow'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot below shows the # of NoShow appointmets over time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts.groupby(['AppointmentDayOrdinal'])['NoShow'].sum(), color=\"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 (Response: Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing set --- hold 1 month of data out as test set based on AppointmentDay\n",
    "df_test = df.copy()\n",
    "# Calculation to find the ordinal value of AppointmentDay to split data at\n",
    "X_test2 = df_test[df['AppointmentDayOrdinal'] >=  (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_test2 = X_test2['IsMale'].values # get the labels we want.\n",
    "\n",
    "if 'IsMale' in X_test2:\n",
    "    del X_test2['IsMale'] # get rid of the class label\n",
    "\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy() \n",
    "                 \n",
    "X_train2 = df_train[df['AppointmentDayOrdinal'] < (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_train2 = X_train2['IsMale'].values # get the labels we want.\n",
    "del X_train2['IsMale'] # get rid of the class label\n",
    "if 'IsMale' in X_train2:\n",
    "    del X_train2['IsMale'] # get rid of the class label\n",
    "\n",
    "#All data\n",
    "df_tmp = df.copy()\n",
    "X2 = df_tmp\n",
    "y2 = df_tmp['IsMale']\n",
    "if 'IsMale' in X2:\n",
    "    del X2['IsMale'] # get rid of the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of Prediciting Features: ', X2.shape)\n",
    "print ('Number of Response: ', y2.shape)\n",
    "print ('Appointment Date Range (Trainig Set): ', min(X_train2['AppointmentDayOrdinal']) ,\" - \",  max(X_train2['AppointmentDayOrdinal']))\n",
    "print ('Number of Training Records: ', \"Predictors: \", X_train2.shape, \"Response:\" ,y_train2.shape)\n",
    "print ('Number of Test Records: ', \"Predictors: \",X_test2.shape, \"Response:\" ,y_test2.shape)\n",
    "print ('Appointment Date Range (Test Set): ', min(X_test2['AppointmentDayOrdinal']) ,\" - \",  max(X_test2['AppointmentDayOrdinal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "<p>A grid search will help determine the optimal parameters to pass to the logistic regression function. Finding the optimal parameters will help with model prediction.</p>\n",
    "<p>Some parameters are being selected as the only option due to the type of dataset. We will try using a solver with a default parameter 'lbfgs' for binomial problems, although sag and saga are faster for larger datasets.</p>\n",
    "<p>The multi-class parameter is set to the default of 'ovr' because we have a binary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X2, y2, cv):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, X2, y2, scoring=['accuracy','precision','recall','f1']\n",
    "                            , cv=cv_object, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "    Recavg = scores['test_recall'].mean()\n",
    "    f1avg = scores['test_f1'].mean()\n",
    "\n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "    print_str3 = \"The average recall for all cv folds is: \\t\\t\\t {Recavg:.5}\"\n",
    "    print_str4 = \"The average f1 for all cv folds is: \\t\\t\\t {f1avg:.5}\"\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print(print_str3.format(Recavg=Recavg))\n",
    "    print(print_str4.format(f1avg=f1avg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "    scoresResults['Recall'] = scores['test_recall']\n",
    "    scoresResults['f1'] = scores['test_f1']\n",
    "\n",
    "    return scoresResults\n",
    "\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X2, y2, cv_object):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, X2, y2, cv=cv_object)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(y2, predictions)\n",
    "    confMat = confusion_matrix(y2, predictions)\n",
    "    acc = accuracy_score(y2, predictions)\n",
    "    \n",
    "    print(classReport)\n",
    "    print(confMat)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='f1'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "LRestimator2 = regGridSearch.best_estimator_\n",
    "LRestimator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = RandomForestClassifier()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='f1'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By performing a grid search, the best parameters to pass into the Logistic Regression are identified for us. Below is the best estimator parameters for class weight and cost without scaling the data. The optimal values were estimated to be 0.01 for C (100 times smaller than the default 1.0 value) and 500 for max_iter (5 times greater than the default value of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "RFestimator2 = regGridSearch.best_estimator_\n",
    "RFestimator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = XGBClassifier()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='f1')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "XGBestimator2 = regGridSearch.best_estimator_\n",
    "XGBestimator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# All\n",
    "\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)),\n",
    "    ('Random Trees', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)),\n",
    "    ('XGBoost', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1))\n",
    "    ]\n",
    "print('All Data')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1], X2, y2)\n",
    "    \n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print('Training Set')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_train2,y_train2)\n",
    "    \n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "print('Testing Set')\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_test2,y_test2)\n",
    "    \n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "\n",
    "#### [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "**Meeting Notes**  Precision, recall, F-measure, ROC, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(LRestimator, X, y, cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(RFestimator, X, y, cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(XGBestimator, X, y, cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(LRestimator2, X2, y2, cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(RFestimator2, X2, y2, cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(XGBestimator2, X2, y2, cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n",
    "\n",
    "#### [10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "\n",
    "#### [10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment (5 points total) \n",
    "\n",
    "#### How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 model - NoShow\n",
    "for our task 1 models, we see logistic regression provide 0 precision or recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total) \n",
    "\n",
    "#### You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "**Meeting Notes** - We did this in MiniLab 1 with chart \n",
    "Possible to add Naives Bayes classification (Luay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
