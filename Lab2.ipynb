{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "## Medical Patient No Show\n",
    "\n",
    "### Team members: Luay Dajani, Dana Geislinger, Chris Morgan, Caroll Rodriguez\n",
    "##### Github - https://github.com/cdmorgan103/7331DataMiningNoShow\n",
    "\n",
    "MSDS 7331, 10/28/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "import seaborn as sns; sns.set(font_scale=1.2)\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "import datetime \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sns\n",
    "\n",
    "# Hide deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Load the data into variable 'df' from pickled object\n",
    "from funcs import load_df\n",
    "df = load_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1\n",
    "#### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "**Meetings Notes: 10/24 ** Chris - copy from MiniLab\n",
    "Variable to remove - AppointmentID & PatientID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKitLearn models have difficulty working with complex objects such as datetime.datetime and datetime.time. To account for this, we will store all dates as ordinal values (the number of days since 1/1/1) and all time values as the number of elapsed seconds in the day (1:30PM = 13 * 3600 + 30 * 60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to ordinal integer values (days since 1/1/1)\n",
    "dt_ord = lambda dt: dt.toordinal()\n",
    "if 'ScheduledDay' in df:\n",
    "    df['ScheduledDayOrdinal'] = df['ScheduledDay'].apply(dt_ord)\n",
    "if 'AppointmentDay' in df:\n",
    "    df['AppointmentDayOrdinal'] = df['AppointmentDay'].apply(dt_ord)\n",
    "\n",
    "# Convert time values to seconds (total seconds since start of day)\n",
    "to_secs = lambda t: t.hour * 3600 + t.minute * 60 + t.second\n",
    "if 'ScheduledTime' in df:\n",
    "    df['ScheduledTimeSeconds'] = df['ScheduledTime'].apply(to_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScheduledDayOrdinal</th>\n",
       "      <th>ScheduledTimeSeconds</th>\n",
       "      <th>AppointmentDayOrdinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>736083</td>\n",
       "      <td>67088</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>736083</td>\n",
       "      <td>58107</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>736083</td>\n",
       "      <td>58744</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736083</td>\n",
       "      <td>62971</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>736083</td>\n",
       "      <td>58043</td>\n",
       "      <td>736083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ScheduledDayOrdinal  ScheduledTimeSeconds  AppointmentDayOrdinal\n",
       "0               736083                 67088                 736083\n",
       "1               736083                 58107                 736083\n",
       "2               736083                 58744                 736083\n",
       "3               736083                 62971                 736083\n",
       "4               736083                 58043                 736083"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['ScheduledDayOrdinal', 'ScheduledTimeSeconds', 'AppointmentDayOrdinal']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110527 entries, 0 to 110526\n",
      "Columns: 115 entries, Age to IsMale\n",
      "dtypes: bool(1), int32(1), int64(10), uint8(103)\n",
      "memory usage: 19.8 MB\n"
     ]
    }
   ],
   "source": [
    "#Remove attributes not usefull\n",
    "del df['PatientId']\n",
    "del df['AppointmentID']\n",
    "\n",
    "# perform one-hot encoding of the categorical data \n",
    "tmp_df = pd.get_dummies(df.Handicap,prefix='Handicap')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.AppointmentDOW,prefix='AppointmentDOW')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.ScheduledDOW,prefix='ScheduledDOW')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.Neighbourhood,prefix='Neighbourhood')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(df.age_range,prefix='age_range')\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Gender attribute with something slightly more intuitive and readable\n",
    "df['IsMale'] = df.Gender=='M' \n",
    "df.IsMale = df.IsMale.astype(np.int)\n",
    "\n",
    "# Now let's clean up the dataset\n",
    "if 'Gender' in df:\n",
    "    del df['Gender'] # if 'Sex' column still exists, delete it (as we created an ismale column)\n",
    "    \n",
    "if 'Handicap' in df:    \n",
    "    del df['Handicap'] # get rid of the original category as it is now one-hot encoded\n",
    "    \n",
    "if 'ScheduledDOW' in df:    \n",
    "    del df['ScheduledDOW'] # get rid of the original category as it is now one-hot encoded\n",
    "    \n",
    "if 'AppointmentDOW' in df:    \n",
    "    del df['AppointmentDOW'] # get rid of the original category as it is now one-hot encoded\n",
    "\n",
    "if 'Neighbourhood' in df:    \n",
    "    del df['Neighbourhood'] # get rid of the original category as it is now one-hot encoded\n",
    "\n",
    "if 'age_range' in df:\n",
    "    del df['age_range']\n",
    "\n",
    "if 'AppointmentID' in df:\n",
    "    del df['AppointmentID']\n",
    "    \n",
    "if 'ScheduledDay' in df:\n",
    "    del df['ScheduledDay']\n",
    "    \n",
    "if 'ScheduledTime' in df:\n",
    "    del df['ScheduledTime']\n",
    "\n",
    "if 'AppointmentDay' in df:\n",
    "    del df['AppointmentDay']\n",
    "\n",
    "# Get an overview of the raw data\n",
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                               int64\n",
      "Scholarship                       int64\n",
      "Hypertension                      int64\n",
      "Diabetes                          int64\n",
      "Alcoholism                        int64\n",
      "SMSReceived                       int64\n",
      "NoShow                             bool\n",
      "DaysInAdvance                     int64\n",
      "ScheduledDayOrdinal               int64\n",
      "AppointmentDayOrdinal             int64\n",
      "ScheduledTimeSeconds              int64\n",
      "Handicap_0                        uint8\n",
      "Handicap_1                        uint8\n",
      "Handicap_2                        uint8\n",
      "Handicap_3                        uint8\n",
      "Handicap_4                        uint8\n",
      "AppointmentDOW_Monday             uint8\n",
      "AppointmentDOW_Tuesday            uint8\n",
      "AppointmentDOW_Wednesday          uint8\n",
      "AppointmentDOW_Thursday           uint8\n",
      "AppointmentDOW_Friday             uint8\n",
      "AppointmentDOW_Saturday           uint8\n",
      "AppointmentDOW_Sunday             uint8\n",
      "ScheduledDOW_Monday               uint8\n",
      "ScheduledDOW_Tuesday              uint8\n",
      "ScheduledDOW_Wednesday            uint8\n",
      "ScheduledDOW_Thursday             uint8\n",
      "ScheduledDOW_Friday               uint8\n",
      "ScheduledDOW_Saturday             uint8\n",
      "ScheduledDOW_Sunday               uint8\n",
      "                                  ...  \n",
      "Neighbourhood_PRAIA DO CANTO      uint8\n",
      "Neighbourhood_PRAIA DO SUÁ        uint8\n",
      "Neighbourhood_REDENÇÃO            uint8\n",
      "Neighbourhood_REPÚBLICA           uint8\n",
      "Neighbourhood_RESISTÊNCIA         uint8\n",
      "Neighbourhood_ROMÃO               uint8\n",
      "Neighbourhood_SANTA CECÍLIA       uint8\n",
      "Neighbourhood_SANTA CLARA         uint8\n",
      "Neighbourhood_SANTA HELENA        uint8\n",
      "Neighbourhood_SANTA LUÍZA         uint8\n",
      "Neighbourhood_SANTA LÚCIA         uint8\n",
      "Neighbourhood_SANTA MARTHA        uint8\n",
      "Neighbourhood_SANTA TEREZA        uint8\n",
      "Neighbourhood_SANTO ANDRÉ         uint8\n",
      "Neighbourhood_SANTO ANTÔNIO       uint8\n",
      "Neighbourhood_SANTOS DUMONT       uint8\n",
      "Neighbourhood_SANTOS REIS         uint8\n",
      "Neighbourhood_SEGURANÇA DO LAR    uint8\n",
      "Neighbourhood_SOLON BORGES        uint8\n",
      "Neighbourhood_SÃO BENEDITO        uint8\n",
      "Neighbourhood_SÃO CRISTÓVÃO       uint8\n",
      "Neighbourhood_SÃO JOSÉ            uint8\n",
      "Neighbourhood_SÃO PEDRO           uint8\n",
      "Neighbourhood_TABUAZEIRO          uint8\n",
      "Neighbourhood_UNIVERSITÁRIO       uint8\n",
      "Neighbourhood_VILA RUBIM          uint8\n",
      "age_range_child                   uint8\n",
      "age_range_adult                   uint8\n",
      "age_range_senior                  uint8\n",
      "IsMale                            int32\n",
      "Length: 115, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Scholarship</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Alcoholism</th>\n",
       "      <th>SMSReceived</th>\n",
       "      <th>DaysInAdvance</th>\n",
       "      <th>ScheduledDayOrdinal</th>\n",
       "      <th>AppointmentDayOrdinal</th>\n",
       "      <th>ScheduledTimeSeconds</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighbourhood_SÃO CRISTÓVÃO</th>\n",
       "      <th>Neighbourhood_SÃO JOSÉ</th>\n",
       "      <th>Neighbourhood_SÃO PEDRO</th>\n",
       "      <th>Neighbourhood_TABUAZEIRO</th>\n",
       "      <th>Neighbourhood_UNIVERSITÁRIO</th>\n",
       "      <th>Neighbourhood_VILA RUBIM</th>\n",
       "      <th>age_range_child</th>\n",
       "      <th>age_range_adult</th>\n",
       "      <th>age_range_senior</th>\n",
       "      <th>IsMale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "      <td>110527.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.089218</td>\n",
       "      <td>0.098266</td>\n",
       "      <td>0.197246</td>\n",
       "      <td>0.071865</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.321026</td>\n",
       "      <td>10.183792</td>\n",
       "      <td>736092.856370</td>\n",
       "      <td>736103.040162</td>\n",
       "      <td>40557.666643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>0.022148</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.234060</td>\n",
       "      <td>0.645598</td>\n",
       "      <td>0.120342</td>\n",
       "      <td>0.350023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23.109921</td>\n",
       "      <td>0.297675</td>\n",
       "      <td>0.397921</td>\n",
       "      <td>0.258265</td>\n",
       "      <td>0.171686</td>\n",
       "      <td>0.466873</td>\n",
       "      <td>15.254924</td>\n",
       "      <td>19.140133</td>\n",
       "      <td>12.189325</td>\n",
       "      <td>11578.064436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127811</td>\n",
       "      <td>0.132541</td>\n",
       "      <td>0.147167</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.037059</td>\n",
       "      <td>0.087409</td>\n",
       "      <td>0.423412</td>\n",
       "      <td>0.478334</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>0.476979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>735912.000000</td>\n",
       "      <td>736083.000000</td>\n",
       "      <td>22176.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>736083.000000</td>\n",
       "      <td>736093.000000</td>\n",
       "      <td>30338.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>736094.000000</td>\n",
       "      <td>736102.000000</td>\n",
       "      <td>37983.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>736104.000000</td>\n",
       "      <td>736115.000000</td>\n",
       "      <td>50382.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>736123.000000</td>\n",
       "      <td>736123.000000</td>\n",
       "      <td>77255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Age    Scholarship   Hypertension       Diabetes  \\\n",
       "count  110527.000000  110527.000000  110527.000000  110527.000000   \n",
       "mean       37.089218       0.098266       0.197246       0.071865   \n",
       "std        23.109921       0.297675       0.397921       0.258265   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%        18.000000       0.000000       0.000000       0.000000   \n",
       "50%        37.000000       0.000000       0.000000       0.000000   \n",
       "75%        55.000000       0.000000       0.000000       0.000000   \n",
       "max       115.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          Alcoholism    SMSReceived  DaysInAdvance  ScheduledDayOrdinal  \\\n",
       "count  110527.000000  110527.000000  110527.000000        110527.000000   \n",
       "mean        0.030400       0.321026      10.183792        736092.856370   \n",
       "std         0.171686       0.466873      15.254924            19.140133   \n",
       "min         0.000000       0.000000       0.000000        735912.000000   \n",
       "25%         0.000000       0.000000       0.000000        736083.000000   \n",
       "50%         0.000000       0.000000       4.000000        736094.000000   \n",
       "75%         0.000000       1.000000      15.000000        736104.000000   \n",
       "max         1.000000       1.000000     179.000000        736123.000000   \n",
       "\n",
       "       AppointmentDayOrdinal  ScheduledTimeSeconds      ...        \\\n",
       "count          110527.000000         110527.000000      ...         \n",
       "mean           736103.040162          40557.666643      ...         \n",
       "std                12.189325          11578.064436      ...         \n",
       "min            736083.000000          22176.000000      ...         \n",
       "25%            736093.000000          30338.000000      ...         \n",
       "50%            736102.000000          37983.000000      ...         \n",
       "75%            736115.000000          50382.000000      ...         \n",
       "max            736123.000000          77255.000000      ...         \n",
       "\n",
       "       Neighbourhood_SÃO CRISTÓVÃO  Neighbourhood_SÃO JOSÉ  \\\n",
       "count                110527.000000           110527.000000   \n",
       "mean                      0.016611                0.017887   \n",
       "std                       0.127811                0.132541   \n",
       "min                       0.000000                0.000000   \n",
       "25%                       0.000000                0.000000   \n",
       "50%                       0.000000                0.000000   \n",
       "75%                       0.000000                0.000000   \n",
       "max                       1.000000                1.000000   \n",
       "\n",
       "       Neighbourhood_SÃO PEDRO  Neighbourhood_TABUAZEIRO  \\\n",
       "count            110527.000000             110527.000000   \n",
       "mean                  0.022148                  0.028337   \n",
       "std                   0.147167                  0.165934   \n",
       "min                   0.000000                  0.000000   \n",
       "25%                   0.000000                  0.000000   \n",
       "50%                   0.000000                  0.000000   \n",
       "75%                   0.000000                  0.000000   \n",
       "max                   1.000000                  1.000000   \n",
       "\n",
       "       Neighbourhood_UNIVERSITÁRIO  Neighbourhood_VILA RUBIM  age_range_child  \\\n",
       "count                110527.000000             110527.000000    110527.000000   \n",
       "mean                      0.001375                  0.007699         0.234060   \n",
       "std                       0.037059                  0.087409         0.423412   \n",
       "min                       0.000000                  0.000000         0.000000   \n",
       "25%                       0.000000                  0.000000         0.000000   \n",
       "50%                       0.000000                  0.000000         0.000000   \n",
       "75%                       0.000000                  0.000000         0.000000   \n",
       "max                       1.000000                  1.000000         1.000000   \n",
       "\n",
       "       age_range_adult  age_range_senior         IsMale  \n",
       "count    110527.000000     110527.000000  110527.000000  \n",
       "mean          0.645598          0.120342       0.350023  \n",
       "std           0.478334          0.325362       0.476979  \n",
       "min           0.000000          0.000000       0.000000  \n",
       "25%           0.000000          0.000000       0.000000  \n",
       "50%           1.000000          0.000000       0.000000  \n",
       "75%           1.000000          0.000000       1.000000  \n",
       "max           1.000000          1.000000       1.000000  \n",
       "\n",
       "[8 rows x 114 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple export of final dataset\n",
    "df.to_csv(\"./data/df.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "#### [5 points] Describe the ﬁnal dataset that is used for classiﬁcation/regression (include a description of any newly formed variables you created).\n",
    "**Meetings Notes 10/24:** Chris - copy from MiniLab\n",
    "\n",
    "**Meeting Notes: ????** Luay wants add a field that ranks the patient's no show history. If they showed up twice and didn't show up 1 would be 33%. Create a temp dataset to group Patient and determine the # of no shows then merge the dataset and build the ranking. \n",
    "\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736111.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['AppointmentDayOrdinal']) - ((max(df['AppointmentDayOrdinal']) - min(df['AppointmentDayOrdinal']))*.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1\n",
    "\n",
    "#### [10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "**Meetings Notes 10/24:** Chris - copy from MiniLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2\t\n",
    "\n",
    "#### [10 points]\tChoose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "**Meeting Notes:** Dana - to figure out error with SMOTE\n",
    "Caroll to run CV on Training Set and evaluate the accuracy of Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partioning Data\n",
    "We want to forecast future NoShows based on Appointment dates. We take the data we have and build a fixed training period in the past up to the last month. The last month of data will be used as testing dataset. Before we deploy the model we will rerun the models utilizing the entire dataset to predict the future outcomes.\n",
    "This approach fits our dataset since we there is correlation between the appoointme date and whether or not they no-show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1(No-Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitPerc = .3 # percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing set --- hold 1 month of data out as test set based on AppointmentDay\n",
    "df_test = df.copy()\n",
    "# Calculation to find the ordinal value of AppointmentDay to split data at\n",
    "X_test = df_test[df['AppointmentDayOrdinal'] >=  (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_test = X_test['NoShow'].values # get the labels we want.\n",
    "\n",
    "if 'NoShow' in X_test:\n",
    "    del X_test['NoShow'] # get rid of the class label\n",
    "\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy() \n",
    "                 \n",
    "X_train = df_train[df['AppointmentDayOrdinal'] < (max(df['AppointmentDayOrdinal'])- \n",
    "             (max(df['AppointmentDayOrdinal'])-(min(df['AppointmentDayOrdinal'])))* splitPerc)]\n",
    "y_train = X_train['NoShow'].values # get the labels we want.\n",
    "del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "if 'NoShow' in X_train:\n",
    "    del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "#All data\n",
    "df_tmp = df.copy()\n",
    "X = df_tmp\n",
    "y = df_tmp['NoShow']\n",
    "if 'NoShow' in X:\n",
    "    del X['NoShow'] # get rid of the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=0.7, n_splits=10)\n"
     ]
    }
   ],
   "source": [
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "seed = 123456789\n",
    "cv_object = TimeSeriesSplit(n_splits=num_cv_iterations,\n",
    "                         max_train_size= 0.7,\n",
    "                         )\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prediciting Features:  (110527, 114)\n",
      "Number of Response:  (110527,)\n",
      "Appointment Date Range (Trainig Set):  736083  -  736109\n",
      "Number of Training Records:  Predictors:  (75283, 114) Response: (75283,)\n",
      "Number of Test Records:  Predictors:  (35244, 114) Response: (35244,)\n",
      "Appointment Date Range (Test Set):  736114  -  736123\n"
     ]
    }
   ],
   "source": [
    "print ('Number of Prediciting Features: ', X.shape)\n",
    "print ('Number of Response: ', y.shape)\n",
    "print ('Appointment Date Range (Trainig Set): ', min(X_train['AppointmentDayOrdinal']) ,\" - \",  max(X_train['AppointmentDayOrdinal']))\n",
    "print ('Number of Training Records: ', \"Predictors: \", X_train.shape, \"Response:\" ,y_train.shape)\n",
    "print ('Number of Test Records: ', \"Predictors: \",X_test.shape, \"Response:\" ,y_test.shape)\n",
    "print ('Appointment Date Range (Test Set): ', min(X_test['AppointmentDayOrdinal']) ,\" - \",  max(X_test['AppointmentDayOrdinal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "<p>A grid search will help determine the optimal parameters to pass to the logistic regression function. Finding the optimal parameters will help with model prediction.</p>\n",
    "<p>Some parameters are being selected as the only option due to the type of dataset. We will try using a solver with a default parameter 'lbfgs' for binomial problems, although sag and saga are faster for larger datasets.</p>\n",
    "<p>The multi-class parameter is set to the default of 'ovr' because we have a binary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X, y, cv):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, X, y, scoring=['accuracy','precision','recall']\n",
    "                            , cv=cv_object, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "    Recavg = scores['test_recall'].mean()\n",
    "\n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "    print_str3 = \"The average recall for all cv folds is: \\t\\t\\t {Recavg:.5}\"\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print(print_str3.format(Recavg=Recavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "    scoresResults['Recall'] = scores['test_recall']\n",
    "\n",
    "    return scoresResults\n",
    "\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X, y, cv_object):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, X, y, cv=cv_object)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(y,predictions)\n",
    "    confMat = confusion_matrix(y,predictions)\n",
    "    acc = accuracy_score(y,predictions)\n",
    "    \n",
    "    print(classReport)\n",
    "    print(confMat)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon researching the possible parameters to LogisticRegression from sklearn, we decided that the most important parameters of interest were C (the inverse of regularization strength, which must be a positive floating point value) and max_iter (the maximum number of iterations for the lbfgs solver to use). The default values for these parameters are 1.0 and 100 respectively, and we have included a range of potential values for these parameters that might help improve our model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-cda288d8de11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m                             )\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#Perform hyperparameter search to find the best combination of parameters for our data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mregGridSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_start\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_starts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_train_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_train_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                 yield (indices[test_start - self.max_train_size:test_start],\n\u001b[0m\u001b[0;32m    798\u001b[0m                        indices[test_start:test_start + test_size])\n\u001b[0;32m    799\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a2b7296e9af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m                             )\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#Perform hyperparameter search to find the best combination of parameters for our data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mregGridSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\MSDS7331\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_start\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_starts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_train_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_train_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                 yield (indices[test_start - self.max_train_size:test_start],\n\u001b[0m\u001b[0;32m    798\u001b[0m                        indices[test_start:test_start + test_size])\n\u001b[0;32m    799\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = RandomForestClassifier()\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy'\n",
    "                            )\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>By performing a grid search, the best parameters to pass into the Logistic Regression are identified for us. Below is the best estimator parameters for class weight and cost without scaling the data. The optimal values were estimated to be 0.01 for C (100 times smaller than the default 1.0 value) and 500 for max_iter (5 times greater than the default value of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "RFestimator = regGridSearch.best_estimator_\n",
    "RFestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "regEstimator = XGBClassifier()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "             }\n",
    "\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diplay the top model parameters\n",
    "XGBestimator = regGridSearch.best_estimator_\n",
    "XGBestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# All\n",
    "\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=0.01, class_weight='none', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2',\n",
    "          random_state=123456789, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "          warm_start=False)),\n",
    "    ('Random Trees',       RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)),\n",
    "    ('XGBoost',XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1))\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X,y)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_train,y_train)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_test,y_test)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SMOTE to perform over-sampling for balancing response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling using SMOTE to balance the NoShows\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy()\n",
    "X_train = df_train\n",
    "\n",
    "y_train = X_train['NoShow'].values # get the labels we want.\n",
    "del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "if 'NoShow' in X_train:\n",
    "    del X_train['NoShow'] # get rid of the class label\n",
    "\n",
    "sm = SMOTE()\n",
    "X_train_ovr, y_train_ovr = sm.fit_sample(X_train, y_train.ravel())\n",
    "print(\"Proportion of response in train set using SMOTE\")\n",
    "for i in np.unique(y_train) :\n",
    "    print(\"The number of {} is {} accouting for {}%.\".format(i, np.bincount(y_train)[i], np.round(np.bincount(y_train)[i]/len(y_train), 3)*100 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.001, 0.01, 0.1, .5, 1, 10,100]\n",
    "              ,'class_weight': [ 'none']\n",
    "              ,'random_state': [123456789]\n",
    "              ,'solver': ['lbfgs']\n",
    "              ,'max_iter':[100,500,1000]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing below that our time series is pretty well stationary over time except for Saturday which show consistantly lower no show appointments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NoShow'] = df['NoShow']\n",
    "df['NoShow_num'] = np.where(df['NoShow']==True, 1, 0)\n",
    "\n",
    "#Set dataframe with index as Appointment Day\n",
    "ts = df[['AppointmentDayOrdinal', 'NoShow'] ]\n",
    "ts = df.set_index('AppointmentDayOrdinal')\n",
    "ts.groupby(['AppointmentDayOrdinal'])['NoShow'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot below shows the # of NoShow appointmets over time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts.groupby(['AppointmentDayOrdinal'])['NoShow'].sum(), color=\"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEstimator = LogisticRegression()\n",
    "#Use the best parameters for our Linear Regression object\n",
    "classifierEst = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(classifierEst, X, y, cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 (Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing set --- hold 1 month of data out as test set based on AppointmentDay\n",
    "df_test = df.copy()\n",
    "\n",
    "X_test = df_test[df['AppointmentDayOrdinal'] >=  dataSplit]\n",
    "y_test = X_test['IsMale'].values # get the labels we want.\n",
    "\n",
    "if 'IsMale' in X_test:\n",
    "    del X_test['IsMale'] # get rid of the class label\n",
    "\n",
    "#Training set -- all records less than appointment date 5/1/2016\n",
    "df_train = df.copy() \n",
    "                 \n",
    "X_train = df_train[df['AppointmentDayOrdinal'] < dataSplit]\n",
    "y_train = X_train['IsMale'].values # get the labels we want.\n",
    "del X_train['IsMale'] # get rid of the class label\n",
    "\n",
    "if 'IsMale' in X_train:\n",
    "    del X_train['IsMale'] # get rid of the class label\n",
    "\n",
    "#All data\n",
    "df_tmp = df.copy()\n",
    "X = df_tmp\n",
    "y = df_tmp['IsMale']\n",
    "if 'IsMale' in X:\n",
    "    del X['IsMale'] # get rid of the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of Prediciting Features: ', X.shape)\n",
    "print ('Number of Response: ', y.shape)\n",
    "print ('Appointment Date Range (Trainig Set): ', min(X_train['AppointmentDayOrdinal']) ,\" - \",  max(X_train['AppointmentDayOrdinal']))\n",
    "print ('Number of Training Records: ', \"Predictors: \", X_train.shape, \"Response:\" ,y_train.shape)\n",
    "print ('Number of Test Records: ', \"Predictors: \",X_test.shape, \"Response:\" ,y_test.shape)\n",
    "print ('Appointment Date Range (Test Set): ', min(X_test['AppointmentDayOrdinal']) ,\" - \",  max(X_test['AppointmentDayOrdinal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "\n",
    "# lets train some trees\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=0.01, class_weight='none', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2',\n",
    "          random_state=123456789, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "          warm_start=False)),\n",
    "    ('Random Trees',RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('XGBoost',XGBClassifier())\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_train,y_train)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "num_estimators = 50\n",
    "# lets train some trees\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=0.01, class_weight='none', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2',\n",
    "          random_state=123456789, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "          warm_start=False)),\n",
    "    ('Random Trees',RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('XGBoost',XGBClassifier())\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X_test,y_test)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# All Data\n",
    "num_estimators = 50\n",
    "#\n",
    "clf_array = [\n",
    "    ('Logistic', LogisticRegression(C=0.01, class_weight='none', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=500,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2',\n",
    "          random_state=123456789, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "          warm_start=False)),\n",
    "    ('Random Trees',       RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('XGBoost',XGBClassifier())\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X,y)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3\n",
    "\n",
    "#### [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "**Meeting Notes**: \n",
    "**Luay (Recursive Feature Elimination for Logistic) here**\n",
    "\t1. Logistic Caroll to move from MiniLab \n",
    "\t2. RandomForest - Chris\n",
    "\t3. XGBoost/Gradient - Caroll\n",
    "\n",
    "Response Gender\n",
    "**Caroll to split training/text with Gender variable**\n",
    "**Luay (Recursive Feature Elimination for Logistic) here**\n",
    "\t1. Logistic Caroll to move from MiniLab\n",
    "\t2. Random -- Chris\n",
    "\t3. XGBoost/Gradient-- Caroll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regEstimator = RandomForestClassifier()\n",
    "\n",
    "\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.001, 0.01, 0.1, .5, 1, 10,100]\n",
    "              ,'class_weight': [ 'none']\n",
    "              ,'random_state': [123456789]\n",
    "              ,'solver': ['lbfgs']\n",
    "              ,'max_iter':[100,500,1000]\n",
    "             }\n",
    "\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4\n",
    "\n",
    "#### [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "**Meeting Notes**  Precision, recall, F-measure, ROC, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5\n",
    "\n",
    "#### [10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6\n",
    "\n",
    "#### [10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby(['AppointmentDay']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.groupby(['AppointmentDay']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment (5 points total) \n",
    "\n",
    "#### How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total) \n",
    "\n",
    "#### You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "**Meeting Notes** - We did this in MiniLab 1 with chart \n",
    "Possible to add Naives Bayes classification (Luay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
